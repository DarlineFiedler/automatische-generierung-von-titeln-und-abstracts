With an inherent prediction of missing high frequency features in bandwidth-expanded speech we also explore the feasibility of adding these estimated features to those extracted from narrowband speech in order to improve the system performance for automatic speech recognition ( ASR ) of narrowband speech .<q>We propose a number of enhancement techniques to improve speech quality in bandwidth expansion ( BWE ) from narrowband to wideband speech , addressing three issues , which could be critical in real-world applications , namely : discontinuity between narrowband spectrum and the estimated high frequency spectrum , energy mismatch between testing and training utterances , and expanding bandwidth of out-of-domain speech signals .<q>Leveraging upon a recently-proposed deep neural network based speech BWE system intended for hearing quality enhancement these techniques not only improve over the traditionally-adopted objective and subjective measures but also reduce the word error rate ( WER ) from 8.67 % when recognizing narrowband speech to 8.26 % when recognizing bandwidth-expanded speech , and almost approaching the WER of 8.12 % when recognizing wideband speech in the 20,000-word open-vocabulary Wall Street Journal ASR task .
Reverberation leads to high word error rates ( WERs ) for automatic speech recognition ( ASR ) systems .<q>We explore different acoustic modeling strategies and language modeling techniques , and demonstrate that robust features with acoustic modeling based on deep learning can provide significant reduction in WERs in the task of recognizing reverberated speech compared to mel-cepstral features and acoustic modeling based on Gaussian Mixture Models ( GMMs ) .
Read-aloud eBooks are a perfect companion for people with reading difficulties and for those learning a new language .<q>In this show and tell description paper we present the read-aloud electronic books ( eBooks ) we create at Sinkronigo.com .<q>Read-aloud eBooks incorporate both audio and text into a single epub3-compliant eBook , which can be read in any compatible eBook reader , where the user can listen to the audio narration while the words are highlighted as spoken .
MIVOQ-PTTS is a new TTS project whose goal is to offer innovative services suitable for creating and using personalized synthetic voices .<q>In this work we will introduce MIVOQ-PTTS main ideas and we will illustrate the current state of development of its first web demonstrator .<q>Users can autonomously create their own synthetic voices by accessing a web interface and recording some sentences ; the voice creation procedure do not require any other human intervention .
Speech quality and intelligibility analysis find triphone models with no grammar , combined with noise adaptation , gives highest performance that outperforms conventional methods of enhancement at low signal-to-noise ratios .<q>This work proposes a method of speech enhancement that uses a network of HMMs to first decode noisy speech and to then synthesise a set of features that enables a clean speech signal to be reconstructed .<q>Different choices of acoustic model ( whole-word , monophone and triphone ) and grammars ( highly constrained to no constraints ) are considered and the effects of introducing or relaxing acoustic and grammar constraints investigated .
We present a scalable , dialog-based conversational practice tool for English language learners that is operationally deployed on the TOEFL® MOOC .<q>The tool consists of three applications of varying duration that recognize the learner 's speech input and responds appropriately .<q>Learners are also provided with basic feedback regarding task performance once complete .
We investigate the recently proposed Time-domain Audio Separation Network ( TasNet ) in the task of real-time single-channel speech dereverberation .<q>Unlike systems that take time-frequency representation of the audio as input , TasNet learns an adaptive front-end in replacement of the time-frequency representation by a time-domain convolutional non-negative autoencoder .<q>We further show that adjusting the stride size in the convolutional autoencoder helps both the dereverberation and separation performance .
The analysis can later be summarized and acted upon to improve or triage the ASR system .<q>The tool presents a webpage consisting of audios and their corresponding references and hypotheses obtained offline .<q>Several other information and features are provided that allow the audios to be categorized and references to be corrected efficiently in a collaborative way almost 10 times faster , without the need for prior knowledge on speech or ASR systems .
Spoofing detection systems for automatic speaker verification have moved from only modelling voiced frames to modelling all speech frames .<q>In this paper , we separate speech into low and high energy frames and independently model the distributions of both to form two spoofing detection systems that are then fused at the score level .<q>Unvoiced speech has been shown to carry information about spoofing attacks and anti-spoofing systems may further benefit by treating voiced and unvoiced speech differently .
Hidden Markov Model ( HMM ) -based synthesis in combination with speaker adaptation has proven to be an approach that is well-suited for child speech synthesis [ 1 ] .<q>The results of the study indicate that gender-independent initial models perform better than gender-dependent initial models and Constrained Structural Maximum a Posteriori Linear Regression ( CSMAPLR ) followed by maximum a posteriori ( MAP ) is the speaker adaptation technique combination that yields the most natural and intelligible synthesized child speech .
A robust lexical mapping function based on the Distributional Semantics paradigm is here proposed as a basic model of grounding language towards the environment .<q>We show that making such information available to the underlying language understanding algorithms improves the accuracy throughout the entire interpretation process .<q>In this work , we aim at demonstrating that such perceptual information ( here modeled through semantic maps ) can be effectively used to enhance the language understanding capabilities of the robot .
Initial experiments with this new system are very promising , achieving state-of-the-art performance for two separate tasks ( Callhome and DIHARD18 ) without any task-dependent parameter tuning .<q>Two problems with this approach are that parameters need significant retuning for different applications , and that the DNN contributes only to the clustering task and not the resegmentation .<q>Many modern systems for speaker diarization , such as the top-performing JHU system in the DIHARD 2018 challenge , rely on clustering of DNN speaker embeddings followed by HMM resegmentation .
In this paper , we investigate single-channel speech enhancement in the recently proposed Double Spectrum ( DS ) framework and provide insights on the statistical properties of speech and noise in the DS domain .<q>While the acoustic frequency domain has been widely used for speech enhancement , usage of the modulation domain is less common .
We investigate the usage of convolutional neural networks ( CNNs ) for the slot filling task in spoken language understanding .<q>Moreover , it combines the information from the past and the future words for classification .<q>Our proposed CNN architecture outperforms even the previously best ensembling recurrent neural network model and achieves state-of-the-art results with an F1-score of 95.61 % on the ATIS benchmark dataset without using any additional linguistic knowledge and resources .
This paper is an acoustic phonetic description of vowels in Hetang Cantonese , and focuses on the diphthongization of nuclear vowels .<q>And , a tetraphthong [ uɔᵄi ] emerges when the nuclear vowel is diphthongized in a triphthong .<q>Different to the representative dialect such as Guangzhou or Hong Kong Cantonese , the Hetang dialect exhibits its unique characteristics regarding the phonetics and phonology of vowels .
This work proposes a process for including priors in speaker diarization with agglomerative hierarchical clustering ( AHC ) .<q>It is also shown that the exclusion of a prior with AHC is itself implicitly a prior , which is found to be geometric growth in the number of speakers .<q>By using more sensible priors , we are able to demonstrate significantly improved robustness to calibration error for speaker counting and speaker diarization .
Based on an acoustic analysis of speech data from 10 speakers , 5 male and 5 female , this paper describes the phonetics and phonology of the vowels and diphthongs in the Xupu Xiang Chinese dialect .<q>Falling diphthongs are composed of a dynamic spectral target , while monophthongs are composed of a static spectral target .<q>But rising diphthongs are sequences of two spectral targets .
With this paper we present an overview of an autarkic embedded cognitive user interface .<q>It is realized in form of an integrated device able to communicate with the user over speech & gesture recognition , speech synthesis and a touch display .<q>Semantic processing and cognitive behaviour control support intuitive interaction and help controlling arbitrary electronic devices .
The idea was initially inspired by ideas from adversarial training , but we show that it can be viewed as a crude way of canceling out certain systematic biases that come from training on finite data sets .<q>It consists of doing two steps for each minibatch : a backward step with a small negative learning rate , followed by a forward step with a larger learning rate .<q>In this paper we describe a modification to Stochastic Gradient Descent ( SGD ) that improves generalization to unseen data .
Recordings of read speech and free response speech before and after exercise in moderate altitude , moderate heat and both moderate altitude and heat are analyzed using features that characterize articulatory coordination .<q>Finally , using cross-validation training of a statistical classifiers , the features are sufficient to classify the four experimental conditions with an overall accuracy of 0.50 and to detect the presence of any one of the experimental conditions with an accuracy of 0.90 .
Our approach is to employ Siamese training in order to obtain a feature representation that minimizes the Euclidean distance between same gender speakers while maximizing it for different gender speakers .<q>For this we choose a challenging scenario where we wish to perform gender recognition but at the same time prevent an attacker who has intercepted the features to perform speaker identification .<q>In this paper we propose a deep neural-network-based feature extraction scheme with the purpose of reducing the privacy risks encountered in speaker classification tasks .
Component models of speech-to-speech translation ( S2S ) systems need to be customized to emerging needs .<q>In this demonstration , we will showcase the technical functionality of BBN 's domain customization tools for S2S systems that allow subject matter experts to augment an existing S2S system with new vocabulary items and translation rules using a web-based user interface .<q>In a recent evaluation of BBN S2S system customized for using these tools , we found 15 % ( relative ) reduction in word error rate as well as 30 % ( relative ) reduction in untranslatable words when used within customized conversational domains .
In order to achieve high performance , we propose an effective acoustic source data structure with a synchronization algorithm .<q>To evaluate the proposed system , series of experiments were conducted in simulated reverberant environments and have shown good performance .<q>The proposed localization system has been implemented to operate in an inaudible frequency range for practical applications .
Furthermore , recurrent neural networks ( RNNs ) incorporating long short-term memory ( LSTM ) cells are adopted to model the complex mapping relationship between the feature sequences describing low-frequency and high-frequency spectra .<q>Experimental results show that the BWE method proposed in this paper can achieve better performance than the conventional method based on Gaussian mixture models ( GMMs ) and the state-of-the-art approach based on DNNs in both objective and subjective tests .<q>In order to utilize linguistic information during the prediction of high-frequency spectral components , the bottleneck ( BN ) features derived from a deep neural network ( DNN ) -based state classifier for narrowband speech are employed as auxiliary input .
Here we examine the articulatory realization of communicative meanings expressed through f0 falling and rising prosodic boundaries in quiet and noisy conditions .<q>Better understanding of the restrictions and affordances this embodiment and situational awareness has on human speech informs the quest for more natural models of human-machine spoken interactions .<q>Human spoken interactions are embodied and situated .
The typology of tone sandhi patterns on disyllabic tonally cognate words in selected sub-groups of the Wu dialects of the east-central Chinese province of Zhejiang is investigated using data from 48 sites collected over 45 years .<q>Five different but typical right-dominant word-tone patterns are identified , acoustically quantified and their geographical distribution specified .<q>It is hypothesized that changes in isolation tones and different types of dissimilation of the first tone from the word-final tone , are a possible origin of the observed variation .
We propose to investigate the development of these differences in production , called idiosyncrasies , by using a Bayesian model of communication .<q>Our experimental results show that only the “ communication model ” provides production idiosyncrasies , suggesting that idiosyncrasies are a natural output of a motor learning process based on a communicative goal .<q>Although speakers of one specific language share the same phoneme representations , their productions can differ .
A supporting system of voice analysis for emergency call centers is being developed at AGH University of Science and Technology in Krakow .<q>The aim of our work is to provide an innovative supporting tool for rapid and accurate assessment of caller profile .<q>The system consists of : speech signal analysis , voiceprints learning , adaptation and classification .
We present a Lipreading system , i.e .<q>a speech recognition system using only visual features , which uses domain-adversarial training for speaker independence .<q>Domain-adversarial training is integrated into the optimization of a lipreader based on a stack of feedforward and LSTM ( Long Short-Term Memory ) recurrent neural networks , yielding an end-to-end trainable system which only requires a very small number of frames of untranscribed target data to substantially improve the recognition accuracy on the target speaker .
Sound event detection with weakly labeled data is considered as a problem of multi-instance learning .<q>Proposed pooling structure has made remarkable improvements on three types of pooling function without adding any parameters .<q>In this paper , we proposed a hierarchical pooling structure to improve the performance of weakly labeled sound event detection system .
This paper presents a novel method which applies Minimum Word Error ( MWE ) training to a Long Short-Term Memory RNN to optimize Voice Activity Detection for speech recognition .<q>The proposed VAD method combining MWE training with RNN yields the best ASR results .<q>Experiments compare speech recognition WERs using RNN VAD with other commonly used VAD methods for two corpora : the conversational Vietnamese corpus used in the NIST OpenKWS13 evaluation and a corpus of French telephone conversations .
We report on a Deep Neural Network frontend for a continuous speech recognizer based on Surface Electromyography ( EMG ) .<q>We show that such a neural network frontend can be trained on EMG data and yields substantial improvements over previous systems , despite the fact that the available amount of data is very small , just amounting to a few tens of sentences : on the EMG-UKA corpus , we obtain average evaluation set Word Error Rate improvements of more than 32 % relative on context-independent phone models and 13 % relative on versatile Bundled Phonetic feature ( BDPF ) models , compared to a conventional system using Gaussian Mixture Models .<q>Speech data is obtained by facial electrodes capturing the electric activity generated by the articulatory muscles , thus allowing speech processing without making use of the acoustic signal .
The efficiency of the proposed method is demonstrated with experiments on balanced training set of Audio set for training and a 5-way test set composed of about 5-hour audio data for testing .<q>we also present an empirical study on confidence measure for few-shot learning application by combining posterior probability with normalized entropy of the network ’ s probability output .<q>Few-shot learning is a very promising and challenging field of machine learning as it aims to understand new concepts from very few labeled examples .
We use a neural network to estimate masks to extract the target speaker and derive beamformer filters using these masks , in a similar way as the recently proposed approach for extraction of speech in presence of noise .<q>Experiments on mixture of two speakers demonstrate that the proposed scheme can track and extract a target speaker for both closed and open speaker set cases .<q>We investigate and compare different methods of passing the speaker information to the network such as making one layer of the network dependent on speaker characteristics .
We propose a speaker diarization system that can incorporate word-level speaker turn probabilities with speaker embeddings into a speaker clustering process to improve the overall diarization accuracy .<q>To integrate lexical and acoustic information in a comprehensive way during clustering , we introduce an adjacency matrix integration for spectral clustering .<q>We show that the proposed method improves diarization performance on various evaluation datasets compared to the baseline diarization system using acoustic information only in speaker embeddings .
We present a multi-task Connectionist Temporal Classification ( CTC ) training for end-to-end ( E2E ) automatic speech recognition with input feature reconstruction as an auxiliary task .<q>These distortions intentionally suppress long-span dependencies in the time domain , which avoids overfitting to the training data .<q>In addition to standard feature reconstruction , we distort the input feature only in the auxiliary reconstruction task , such as ( 1 ) swapping the former and latter parts of an utterance , or ( 2 ) using a part of an utterance by stripping the beginning or end parts .
The task is to measure how accurately one can detect 1 ) whether a test recording is spoken by a blacklisted speaker , and 2 ) which specific blacklisted speaker was talking .<q>The Multi-target Challenge aims to assess how well current speech technology is able to determine whether or not a recorded utterance was spoken by one of a large number of blacklisted speakers .<q>It is a form of multi-target speaker detection based on real-world telephone conversations .
We implement this process with the Bayesian Subspace Hidden Markov Model ( SHMM ) , a model akin to the Subspace Gaussian Mixture Model ( SGMM ) where each low dimensional embedding represents an acoustic unit rather than just a HMM ’ s state .<q>Our approach may be described by the following two steps procedure : first the model learns the notion of acoustic units from the labelled data and then the model uses its knowledge to find new acoustic units on the target language .<q>The subspace is trained on 3 languages from the GlobalPhone corpus ( German , Polish and Spanish ) and the AUs are discovered on the TIMIT corpus .
We propose a fully convolutional sequence-to-sequence encoder architecture with a simple and efficient decoder .<q>Key to our approach is a time-depth separable convolution block which dramatically reduces the number of parameters in the model while keeping the receptive field large .<q>Our model improves WER on LibriSpeech while being an order of magnitude more efficient than a strong RNN baseline .
In this paper , we describe a system for automatically obtaining pronunciations of words for which pronunciations are not available , but for which transcribed data exists .<q>Speech recognition systems for irregularly-spelled languages like English normally require hand-written pronunciations .<q>Experiments on various ASR tasks show that , with the proposed framework , starting with an initial lexicon of several thousand words , we are able to learn a lexicon which performs close to a full expert lexicon in terms of WER performance on test data , and is better than lexicons built using G2P alone or with a pruning criterion based on pronunciation probability .
This was done by designing a new definition of the output signal-to-noise ratio to compensate the biased estimation of the input SNR prior to the noise-suppression processing .<q>In this study , the normalized covariance metric ( NCM ) , a STI-based intelligibility measure , was modified to reduce the effects of non-linear distortions introduced by most noise-suppression algorithms for intelligibility prediction .<q>Significantly higher correlation with intelligibility score was obtained from the modified NCM measure , in contrast to those from the original NCM measure .
Extralinguistic conditioning identified in this study corroborates findings reported by [ 1 ] : females are more prone to squeaking and the phenomenon is individual-dependent .<q>At present , nothing is known about the conditioning and the articulation of this feature of speech .<q>In this interpretation , we hypothesise that squeaks occasionally occur during constriction disengagement : at the point when VVFC suddenly releases but the TAs have not yet fully relaxed .
We design a novel deep learning framework for multi-channel speech recognition in two aspects .<q>Testing on the CHiME-4 Challenge speech recognition task with a single set of acoustic and language models , our approach achieves the best performance of all three tracks ( 1-channel , 2-channel , and 6-channel ) among submitted systems .<q>First , for the front-end , an iterative mask estimation ( IME ) approach based on deep learning is presented to improve the beamforming approach based on the conventional complex Gaussian mixture model ( CGMM ) .
Non-negative Matrix Factorization ( NMF ) has already been applied to learn speaker characterizations from single or non-simultaneous speech for speaker recognition applications .<q>It is also known for its good performance in ( blind ) source separation for simultaneous speech .<q>It is shown how state-of-the-art multichannel NMF for blind source separation can be easily extended to incorporate speaker recognition .
A second contribution is a comparison between SNM and the related class of Maximum Entropy language models .<q>Being able to train on held-out data is very important in practical situations where training data is mismatched from held-out/test data .<q>While much cheaper computationally , we show that SNM achieves slightly better perplexity results for the same feature set and same speech recognition accuracy on voice search and short message dictation .
We investigated how English rhythmic patterns develop in the course of first language acquisition by children between four and twelve years .<q>This finding indicates that the language-specific phonetic timing patterns are established as a function of age , probably as a result of the motor control development .<q>We have empirically confirmed that rhythm becomes increasingly more stress-timed as acquisition progresses , which is revealed by higher durational variability of syllables , vocalic sequences and consonantal clusters in speech delivered by older children compared to younger children .
We also propose a method of aligning sets of files which uses the cumulative evidence from previous alignments to help align the weakest matches .<q>We propose an adaptive audio fingerprint which is learned on-the-fly in a completely unsupervised manner to adapt to the characteristics of a given set of unaligned recordings .<q>Based on challenging alignment scenarios extracted from the ICSI meeting corpus , the proposed alignment system is able to achieve > 99 % alignment accuracy at a 100ms error tolerance .
The output of the emotion analysis is visualized graphically in the arousal and valence space alongside the emotion category and further speaker characteristics .<q>A show-case application for the Android platform is provided , where audEERING ’ s highly noise robust voice activity detection based on LSTM-RNN is combined with our core emotion recognition and speaker characterisation engine natively on the mobile device .<q>This eliminates the need for network connectivity and allows to perform robust speaker state and trait recognition efficiently in real-time without network transmission lags .
It is difficult to apply well-formulated model-based noise adaptation approaches to Deep Neural Network ( DNN ) due to the lack of interpretability of the model parameters .<q>In this paper , we propose incorporating a generative front-end layer ( GFL ) , which is parameterised by Gaussian Mixture Model ( GMM ) , into the DNN .<q>A GFL can be easily adapted to different noise conditions by applying the model-based Vector Taylor Series ( VTS ) to the underlying GMM .
We introduce a technique for dynamically applying contextually-derived language models to a state-of-the-art speech recognition system .<q>These generally small-footprint models can be seen as a generalization of cache-based models [ 1 ] , whereby contextually salient n-grams are derived from relevant sources ( not just user generated language ) to produce a model intended for combination with the baseline language model .<q>The derived models are applied during first-pass decoding as a form of on-the-fly composition between the decoder search graph and the set of weighted contextual n-grams .
We demonstrate Visual Learning 2 , an English pronunciation app for second-language ( L2 ) learners and phonetics students .<q>This iOS app links together audio , front and side video , MRI and ultrasound movies of a native speaker reading a phonetically balanced text .<q>Users can record their own audio/video and play it back in sync with the model for comparison .
This paper presents a multi-task learning architecture for training the speaker embedding DNN with the primary task of classifying the target speakers , and the auxiliary task of reconstructing the first- and higher-order statistics of the original input utterance .<q>The x-vector based deep neural network ( DNN ) embedding systems have demonstrated effectiveness for text-independent speaker verification .<q>The proposed training strategy aggregates both the supervised and unsupervised learning into one framework to make the speaker embeddings more discriminative and robust .
Automatic speech recognition ( ASR ) systems often need to be developed for extremely low-resource languages to serve end-uses such as audio content categorization and search .<q>This paper presents our Kaldi-based systems for the program , which employ a universal phone modeling approach to ASR and describes recipes for very rapid adaptation of this universal ASR system .<q>While universal phone recognition is natural to consider when no transcribed speech is available to train an ASR system in a language , adapting universal phone models using very small amounts ( minutes rather than hours ) of transcribed speech also needs to be studied , particularly with state-of-the-art DNN-based acoustic models .
To deal with heterogenous schemas , we introduce a simple data-driven method for transforming the candidate slots .<q>In large-scale multi-domain systems , this presents two challenges - scaling to a very large and potentially unbounded set of slot values and dealing with diverse schemas .<q>Our experiments show that our approach can scale to multiple domains and provides competitive results over a strong baseline .
We present an effective method to solve a small-footprint keyword spotting ( KWS ) task via deep neural network for mobile game .<q>To this end , we propose a new neural network layer named recycle-pooling .<q>We will perform live demonstration of RP-CNN based KWS integrated into a full-sized , production-quality mobile game A3 : Still Alive , which is one of the major games from Netmarble this year and will be available on market soon .
However , due to the large dimension of raw audio waveforms , pooling layers are usually used aggressively between temporal convolutional layers .<q>Previous work has shown that deep convolutional neural networks ( CNNs ) as front-end can learn effective representations from the raw waveform .<q>In essence , these pooling layers perform operations that are similar to signal downsampling , which may lead to temporal aliasing according to the Nyquist-Shannon sampling theorem .
We argue that the transcription task is a simpler and more practical way of collecting annotations which also leads to more valid data for training an automatic scoring system .<q>We compare the error detection task to a simple transcription task in which the annotators were asked to transcribe the same fragments using standard English spelling .<q>This paper evaluates and compares different approaches to collecting judgments about pronunciation accuracy of non-native speech .
Here , we extend the BoAW feature extraction process with the use of Deep Neural Networks : first we train a DNN acoustic model on an acoustic dataset consisting of 22 hours of speech for phoneme identification , then we evaluate this DNN on a standard paralinguistic dataset .<q>The Bag-of-Audio-Word ( or BoAW ) representation is an utterance-level feature representation approach that was successfully applied in the past in various computational paralinguistic tasks .<q>To construct utterance-level features from the frame-level posterior vectors , we calculate their BoAW representation .
The system currently supports translation from Arabic to English .<q>QAT2 is a multimedia content translation web service developed by QCRI to help content provider to reach audiences and viewers speaking different languages .<q>The result is a complete native language experience for end users on foreign language websites .
We developed Emojive ! , a mobile game app to make emotion recognition from audio and image interactive and fun , motivating the users to play with the app .<q>The more users play the game , the more emotion-labelled data will be acquired .<q>The game is to act out a specific emotion , among six emotion labels ( happy , sad , anger , anxiety , loneliness , criticism ) , given by the system .
Efficiency of the proposed technique is experimentally validated on the recent NIST 2016 and 2018 Speaker Recognition Evaluation datasets .<q>Any shift between training and test data , in terms of device , language , duration , noise or other tends to degrade accuracy of speaker detection .<q>Details and relevance of different approaches are described and commented , leading to a new robust method that we call feature-Distribution Adaptor .
Learning an acoustic model directly from the raw waveform has been an active area of research .<q>We will show that raw waveform features match the performance of log-mel filterbank energies when used with a state-of-the-art CLDNN acoustic model trained on over 2,000 hours of speech .<q>Specifically , we will show the benefit of the CLDNN , namely the time convolution layer in reducing temporal variations , the frequency convolution layer for preserving locality and reducing frequency variations , as well as the LSTM layers for temporal modeling .
In this paper we present our works towards creating a natural language platform for an intelligent driving assistant ( IDA ) for smart parking in Singapore .<q>In particular , we are focusing on the challenges of designing and implementing reliable spoken dialogue components that enable drivers to communicate hands-free with the system .<q>These components require : spoken language dialogue design , data collection , as well as training of speech recognition ( ASR ) and natural language understanding ( NLU ) modules .
The filter-bank outputs with temporal contexts form a time-frequency pattern of speech and have been shown to be effective as a feature parameter for DNN-based acoustic models .<q>Experimental comparisons carried out in phoneme recognition demonstrate that the tensor feature provides comparable results to the filter-bank feature , and the fusion of the two features yields an improvement over each feature .<q>Filter-bank outputs are extended into tensors to yield precise acoustic features for speech recognition using deep neural networks ( DNNs ) .
We utilized data of 142 individuals undergoing depression screening and modeled the interactions with audio and text features in a Long-Short Term Memory ( LSTM ) neural network model to detect depression .<q>Like professionals , an effective automated agent must understand that responses to queries have varying prognostic value .<q>Our results were comparable to methods that explicitly modeled the topics of the questions and answers which suggests that depression can be detected through sequential modeling of an interaction , with minimal information on the structure of the interview .
In this paper , integrated with the power spectra and inter-channel spatial features at the input level , we explore to leverage directional features , which imply the speaker source from the desired target direction , for target speaker separation .<q>We demonstrate , on the far-field WSJ0 2-mix dataset , that our proposed approach significantly improves the performance of speech separation against the baseline single-channel and multi-channel speech separation methods .<q>In addition , we incorporate an attention mechanism to dynamically tune the model ’ s attention to the reliable input features to alleviate spatial ambiguity problem when multiple speakers are closely located .
Semi-supervised and cross-lingual knowledge transfer learnings are two strategies for boosting performance of low-resource speech recognition systems .<q>Such a knowledge transfer learning is realized by fine-tuning of Deep Neural Network ( DNN ) .<q>We then combine these two learning strategies to obtain further performance improvement .
The measure for model quality is the correlation of phoneme recognition accuracies obtained in ASR and in human speech recognition ( HSR ) .<q>Phoneme-specific response rates are obtained from ASR based on deep neural networks ( DNNs ) and from listening tests with six normal-hearing subjects .<q>Various feature representations are used as input to the DNNs to explore their relation to overall ASR performance and model prediction power .
The model includes a comprehensive description of the upper airway musculature , using point-to-point muscles that may either be embedded within the deformable structures or operate externally .<q>We demonstrate that the biomechanics , in conjunction with the skinning , supports a range from physically realistic to simplified vocal tract geometries to investigate different approaches to aeroacoustic modeling of vocal tract .<q>We introduce a biomechanical model of oropharyngeal structures that adds the soft-palate , pharynx , and larynx to our previous models of jaw , skull , hyoid , tongue , and face in a unified model .
We focus our analysis on the application of two standard methodologies , based on Hidden Markov Models ( HMMs ) and Deep Neural Networks ( DNNs ) , respectively , to train both acoustic models and the tongue model parameter weights .<q>The results show that even with less than 2h of data , DNNs already outperform HMMs .<q>We evaluate both methodologies at every step by comparing the predicted articulatory movements against the reference data .
However , one limitation of CNNs is that , with max-pooling , they do not consider the pose relationship between low-level features .<q>Motivated by this problem , we apply the capsule network to capture the spatial relationship and pose information of speech spectrogram features in both frequency and time axes .<q>We show that our proposed end-to-end SR system with capsule networks on one-second speech commands dataset achieves better results on both clean and noise-added test than baseline CNN models .
In this paper , we propose to combine VoiceLoop , an autoregressive SS model , with Variational Autoencoder ( VAE ) .<q>Recent advances in neural autoregressive models have improve the performance of speech synthesis ( SS ) .<q>Experiments using the VCTK and Blizzard2012 datasets show the VAE helps VoiceLoop to generate higher quality speech and to control the experssions in its synthesized speech by incorporating global characteristics into the speech generating process .
This paper proposes a novel approach to create a unit set for CTC-based speech recognition systems .<q>We investigate both Crossword units , that may span multiple word and Subword units .<q>In contrast to using characters or words as units this allows us to find a good trade-off between the size of our unit set and the available training data .
CQCC features are extracted with the constant Q transform ( CQT ) , a perceptually-inspired alternative to Fourier-based approaches to time-frequency analysis .<q>A comparative assessment of CQCCs and mel frequency cepstral coefficients ( MFCC ) for a short-duration speaker verification scenario shows that CQCCs generally outperform MFCCs and that the two feature representations are highly complementary ; fusion experiments with the RSR2015 and RedDots databases show relative reductions in equal error rates of as much as 60 % compared to an MFCC baseline .<q>The CQT offers greater frequency resolution at lower frequencies and greater time resolution at higher frequencies .
We explore whether this information can be exploited in predicting incipient speech activity .<q>Our experiments show that respiratory information substantially lowers cross-entropy rates , and that this generalizes to unseen data .<q>Using a methodology called stochastic turn-taking modeling , we compare the performance of a model trained on speech activity alone to one additionally trained on static and dynamic lung volume features .
Therefore mixed-bandwidth ( MB ) acoustic modeling has important practical values for ASR system deployment .<q>We study various MB strategies including downsampling , upsampling and bandwidth extension for MB acoustic modeling and evaluate their performance on 8 diverse WB and NB test sets from various application domains .<q>To deal with the large amounts of training data , distributed training is carried out on multiple GPUs using synchronous data parallelism .
A statistical analysis of the prediction error vector at the DNN output reveals that it follows a unimodal density for each log power spectral component .<q>Our experiments on the Speech Separation Challenge ( SSC ) corpus show that the proposed learning approach can achieve a better generalization capability and a faster convergence than MMSE-based DNN learning .<q>Furthermore , we demonstrate that the ML-trained DNN consistently outperforms MMSE-trained DNN in all the objective measures of speech quality and intelligibility in single-channel speech separation .
Allophonic height alternations in MOUTH and PRICE , conditioned by coda voicing , have been confirmed for a wide variety of English accents , especially in the North American context .<q>The results of an acoustic analysis of 15 creole speakers from Nassau , Bahamas , support the claim that automatic phonetic processes may underly the initial direction of the allophony , which is then available for sociolinguistic differentiation .<q>Realisations may vary in degree but not in direction .
The goal of SpeechYOLO is to localize boundaries of utterances within the input signal , and to correctly classify them .<q>More specifically , we present SpeechYOLO , which is inspired by the YOLO algorithm [ 1 ] for object detection in images .<q>In this paper , we propose to apply object detection methods from the vision domain on the speech recognition domain , by treating audio fragments as objects .
The audio-visual speech recognition system we present in this paper is for a person with severe hearing loss in noisy environments .<q>We propose a novel visual feature extraction approach that connects the lip image to audio features efficiently , and the use of convolutive bottleneck networks ( CBNs ) increases robustness with respect to speech fluctuations caused by hearing loss .<q>Although feature integration is an important factor in multimodal speech recognition , it is difficult to integrate efficiently because those features are different intrinsically .
The experimental results show that hybrid CNN+RNN architecture is beneficial for learning long-term patterns in spectrogram exhibited by koalas ' calls in unseen conditions .<q>The proposed method is also applicable for detecting other animal calls such as bird sound where it achieves 87.46 % area under curve score on the bird audio detection challenge evaluation data .<q>The benefit of this architecture is twofold : firstly , convolutional layers learn local time-frequency patterns from the audio spectrogram and secondly , recurrent layers model longer temporal dependencies of the extracted features .
Transcription was performed and labelling was carried out in seven emotional categories , as well as for the intelligibility of the speaker .<q>However , to the best of the authors knowledge , none of these datasets consist solely of emotional speech samples from individuals with mental , neurological and/or physical disabilities .<q>With the aim of advancing these technologies , we fill this void in emotional speech resources by introducing the EmotAsS ( Emotional Sensitivity Assistance System for People with Disabilities ) corpus consisting of spontaneous emotional German speech data recorded from 17 mentally , neurologically and/or physically disabled participants in their daily work environment , resulting in just under 11 hours of total speech time and featuring approximately 12.7 k utterances after segmentation .
Deep CNN architecture , which has attracted great attention in various research areas , has also been successfully applied to ASR .<q>In this paper , we present a framework of a factored deep convolutional neural network ( CNN ) learning for noise robust automatic speech recognition ( ASR ) .<q>The proposed factored deep CNN framework factors out feature enhancement , delta parameter learning , and hidden Markov model state classification into three specific network blocks .
Our method results in an additional 3 % error reduction over prior schemes that use classification networks , and we use 36 % fewer network parameters .<q>The use of diffuse reflection results in 34 % and 41 % reduction in angular prediction errors on LOCATA and SOFA datasets , respectively , over prior methods based on image-source methods .<q>We present a novel learning-based approach to estimate the direction-of-arrival ( DOA ) of a sound source using a convolutional recurrent neural network ( CRNN ) trained via regression on synthetic data and Cartesian labels .
In this paper , we investigate an efficient method of encoding multilingual transcriptions for training E2E ASR systems .<q>However , constructing the multilingual low-resource E2E ASR system is still challenging due to the vast number of symbols ( e.g. , words and characters ) .<q>Compared with traditional multilingual modeling methods , we directly build a single acoustic-articulatory within recent transformer-based E2E framework for ASR tasks .
Extemporaneous speech is a delivery type in public speaking which uses a structured outline but is otherwise delivered conversationally , off the cuff .<q>We resynthesised the beginnings of two Interspeech keynote speeches with TTS that produces multiple different versions of each utterance that vary in fluency and filled-pause placement .<q>The platform allows the user to mark the samples according to any perceptual aspect of interest , such as certainty , authenticity , confidence , etc .
Machine learning ( ML ) models - like deep neural networks - require substantial amounts of training data .<q>By using data workflows adapted for speech technologies and natural language processing systems , the user can collect and enrich speech and text data .<q>This paper describes a platform designed to create high-quality datasets .
PATSY is an abbreviation for its German name “ Piloten/ATC Trainingssystem für den Sprechfunk ” which translates to pilot/ air traffic control ( ATC ) training system for radio communication .<q>At the Show and Tell session at Interspeech 2015 , we would like to present a subsystem of PATSY in which we only concentrate on pronunciation scoring .<q>The speaker is prompted to record a sequence of words of the NATO phonetic alphabet and he is given back a visually enhanced feedback regarding his pronunciation score .
These perceptual differences between the Mandarin and Korean groups might be attributed to the different acoustic distribution in the F1×F2 vowel space of the two different native languages .<q>Furthermore , the Mandarin listeners tended to label stimuli more often as /a/ and less often as /u/ than the Korean counterparts .<q>The results showed that both language groups exhibited categorical features in vowel perception , with a sharper categorical boundary of /ɜ/-/u/ than that of /a/-/ɜ/ .
Sonorant segmentation of speech signals is critical in developing Automatic Speech Recognition ( ASR ) systems , audio search systems and for automatic segmentation of speech corpora .<q>In this work , acoustic features based on excitation source and vocal tract system characteristics of sonorant sounds are proposed for segmentation of sonorant regions in continuous speech .<q>TIMIT database is used to test the validity and AMI meeting corpus and Telugu ( an Indian language ) dataset are considered to test the utility of the proposed features .
In this paper we consider the problem of fingerspelling recognition in videos , introducing an end-to-end lexicon-free model that consists of a deep auto-encoder image feature learner followed by an attention-based encoder-decoder for prediction .<q>Although fingerspelling is an often overlooked component of sign languages , it has great practical value in the communication of important context words that lack dedicated signs .<q>The feature extractor is a vanilla auto-encoder variant , employing a quadratic activation function .
In this paper , we proposed Random Speaker-variability Subspace ( RSS ) projection to map a data into LSH based hash tables .<q>This paper describes a fast speaker search system to retrieve segments of the same voice identity in the large-scale data .<q>Multiple RSS can be generated by randomly selecting a subset of speakers from a large speaker cohort .
We use ABX discrimination tasks to evaluate the discriminability and invariance properties of the obtained joined embeddings , and compare these results with mono-embeddings architectures .<q>We also compare the standard siamese networks fed with same ( AA ) or different ( AB ) pairs , to a ‘ triamese ’ network fed with AAB triplets .<q>Here , we scale up these architectures to the 360 hours of the Librispeech corpus by implementing a sampling method to efficiently select pairs of words from the dataset and improving the loss function .
We propose and evaluate the use of an affective-semantic model to expand the affective lexica of German , Greek , English , Spanish and Portuguese .<q>Motivated by the assumption that semantic similarity implies affective similarity , we use word level semantic similarity scores as semantic features to estimate their corresponding affective scores .<q>We achieve classification accuracy ( valence polarity task ) between 85 % and 91 % for all five languages .
The corpus is freely available for download from http : //www.openslr.org/60/ .<q>This paper introduces a new speech corpus called “ LibriTTS ” designed for text-to-speech use .<q>It is derived from the original audio and text materials of the LibriSpeech corpus , which has been used for training and evaluating automatic speech recognition systems .
Reframing Automatic Diacritic Restoration ( ADR ) as a machine translation task , we experiment with two different attentive Sequence-to-Sequence neural models to process undiacritized text .<q>Diacritics provide morphological information , are crucial for lexical disambiguation , pronunciation and are vital for any Yorùbá text-to-speech ( TTS ) , automatic speech recognition ( ASR ) and natural language processing ( NLP ) tasks .<q>With very few exceptions , diacritics are omitted from electronic texts , due to limited device and application support .
As Gaussian component level polynomial interpolation is performed for each high dimensional DNN bottleneck feature vector at a frame level , conventional GVP-HMMs are computationally expensive to use in recognition time .<q>Recently a new approach to incorporate deep neural networks ( DNN ) bottleneck features into HMM based acoustic models using generalized variable parameter HMMs ( GVP-HMMs ) was proposed .<q>Consistent improvements were also obtained on other subsets .
This outperforms Deep Speech 2 , the best reported character-based system in the literature while using three orders of magnitude less labeled training data .<q>wav2vec is trained on large amounts of unlabeled audio data and the resulting representations are then used to improve acoustic model training .<q>We explore unsupervised pre-training for speech recognition by learning representations of raw audio .
This paper studies topic and keyword identification for languages in which we have no transcribed speech data .<q>We also discuss the successful detection of topic dependent keywords and the use of unsupervised learning based clusters in our approach for low-resourced language topic detection .<q>Specifically , we propose that a convolutional neural network ( CNN ) trained as a topic classifier in an RRL learns features ( hidden layer activations ) that can be used for the same purpose in an LRL .
Replay attack poses a key threat for automatic speaker verification systems .<q>Spoofing detection systems inspired by auditory perception have shown promise to date , however some aspects of auditory processing have not been investigated in this context .<q>Evaluation on the ASVspoof 2017 version 2 database suggests that the adaptive-Q compression provided by the proposed model helps to improve the performance of replay detection , and a relative reduction in EER of 26 % was achieved compared with the best results reported for auditory system-based feature proposed for replay attack detection .
Unfortunately , the precision of existing techniques for phone-level lyrics-to-audio alignment has been found insufficient for this task .<q>Speech-to-Singing refers to techniques that transform speech to a singing voice .<q>A major performance factor of this process relies on the precision to align the phonetic sequence of the input speech to the timing of the target singing .
This paper describes the systems developed by the JHU team for the speaker recognition track of the 2019 VOiCES from a Distance Challenge .<q>A fusion of all three x-vector systems was our primary submission , and it obtained an actual DCF of 0.362 .<q>On this far-field task , we achieved good performance using systems based on state-of-the-art deep neural network ( DNN ) embeddings .
We propose a multi-channel training framework for the deep speaker embedding neural network on noisy and reverberant data .<q>In this study , we focus on far-field speaker recognition with a microphone array .<q>Despite the significant improvements in speaker recognition enabled by deep neural networks , unsatisfactory performance persists under far-field scenarios due to the effects of the long range fading , room reverberation , and environmental noises .
The results show that beginnings of lexical knowledge may indeed emerge from individually ambiguous learning scenarios .<q>The model is tested on recordings of real infant-caregiver interactions using utterance-level labels for concrete visual objects that were attended by the infant when caregiver spoke an utterance containing the name of the object , and using random visual labels for utterances during absence of attention .<q>However , feasibility of this hypothesis in terms of real-world infant experiences has remained unclear .
Individual birds can recognise familiar individuals from calls , but where in the signal is this identity encoded ?<q>Bird calls range from simple tones to rich dynamic multi-harmonic structures .<q>We studied the question by applying a combination of feature representations to a dataset of jackdaw calls , including linear predictive coding ( LPC ) and adaptive discrete Fourier transform ( aDFT ) .
These include end-to-end DNN regression from noisy to clean spectra , as well as less intervening approaches which estimate a suppression gain for each time-frequency bin instead of directly recovering the clean spectral features .<q>We study an alternative data-driven approach which uses deep neural networks ( DNNs ) to learn the transformation from noisy and reverberant speech to clean speech , with a focus on real-time applications which require low-latency causal processing .<q>It is shown that DNN-based suppression gain estimation outperforms the regression approach in the causal processing mode and for noise types that are not seen during DNN training .
We investigate a number of Deep Neural Network ( DNN ) architectures for emotion identification with the IEMOCAP database .<q>First we compare different feature extraction front-ends : we compare high-dimensional MFCC input ( equivalent to filterbanks ) , versus frequency-domain and time-domain approaches to learning filters as part of the network .<q>Next we investigated different ways to aggregate information over the duration of an utterance .
In this paper , we propose to learn a novel representation ‘ Bag-of-Context-Aware-Words ’ that encapsulates the context with neighbouring frames of BoAW ; segment-level BoAW are extracted in the first layer which are then utilised to create a final instance-level bag .<q>The results show that , the best segment length for valence is twice of that for arousal , suggesting that the prediction of the emotional valence requires more context information than the prediction of arousal and the performance obtained on RECOLA with the proposed Bag-of-Context-Aware-Words outperforms all previously reported results .<q>Whereas systems based on deep learning have been proposed to learn efficient representations of emotional speech data , methods such as Bag-of-Audio-Words ( BoAW ) have yielded similar or even better performance while providing understandable representations of the data .
While DNN-HMM acoustic models have replaced GMM-HMMs in the standard ASR pipeline due to performance improvements , one unrealistic assumption that remains in these models is the conditional independence assumption of the Hidden Markov Model ( HMM ) .<q>In this work , we explore the extent to which depth of neural networks helps compensate for these poor conditional independence assumptions .<q>That performance substantially degrades , however , as the data becomes more realistic suggests that better temporal modeling is still needed for ASR .
To overcome the lack of data available for this spoken language understanding approach , we investigate the use of a transfer learning strategy based on the principles of curriculum learning .<q>Experiments are carried out on the French MEDIA and PORTMEDIA corpora and show that this end-to-end SLU approach reaches the best results ever published on this task .<q>We compare our approach to a classical pipeline approach that uses ASR , POS tagging , lemmatizer , chunker … and other NLP tools that aim to enrich ASR outputs that feed an SLU text to concepts system .
Yet , it is easily learned by speakers of the language that is being whistled , as attested by current efforts of revitalization of whistled Spanish in the Canary Islands .<q>To understand better the relation between whistled and spoken speech perception , we looked here at how Spanish native speakers knowing nothing about whistled speech categorized four Spanish whistled vowels .<q>Whistled speech in a non tonal language consists of the natural emulation of vocalic and consonantal qualities in a simple modulated whistled signal .
Preliminary experiments suggest that mimicry can be detected using this methodology by measuring how much speakers converge or diverge with respect to one another in terms of acoustic evidence .<q>The tendency to unconsciously imitate others in conversations has been referred to as mimicry , accommodation , interpersonal adaptation , etc .<q>Given that mimicry is the unconscious tendency to imitate others , this article proposes the adoption of speaker verification methodologies that were originally conceived to spot people trying to forge the voice of others .
In this paper , we present an evolving learning framework to automate the design of neural network structures for infant vocalisation analysis .<q>Infant vocalisation analysis plays an important role in the study of the development of pre-speech capability of infants , while machine-based approaches nowadays emerge with an aim to advance such an analysis .<q>This framework consists of a controller and its child networks , where the child networks are built according to the controller 's estimation .
Unstressed vowels in Russian are reduced in both duration and quality , but these two manifestations of vowel reduction do not have to be observed simultaneously .<q>In order to investigate this question , we analysed the reduction pattern of words in such contexts where lengthening is induced by prosodic factors : prominence and pre-boundary lengthening .<q>Additionally , these results may serve as an argument for the idea that the two processes — vowel reduction and temporal organization of utterance — are autonomous .
Phonetic information , as an important component of speech , is rarely considered in the extraction of speaker embeddings .<q>Speaker embeddings achieve promising results on many speaker verification tasks .<q>In this paper , we introduce phonetic information to the speaker embedding extraction based on the x-vector architecture .
Stimuli were taken from a French-German corpus of read speech with German native speakers as raters .<q>This backs the cliché that French has an attractive image for German listeners .<q>An analysis of the best vs. the worst rated sentences suggest that an individual mix of voice quality , disfluency management , prosodic behaviour and pronunciation precision is responsible for the results .
In this paper , we propose an automatic approach for the prediction of dysarthric speech evaluation metrics ( intelligibility , severity , articulation impairment ) based on the representation of the speech acoustics in the total variability subspace based on the i-vectors paradigm .<q>The proposed approach , evaluated on 129 French dysarthric speakers from the DesPhoAPady and VML databases , is proven to be efficient for the modeling of patient ’ s production and capable of detecting the evolution of speech quality .<q>As a result , such technologies have been exploited in new areas and applications including medical practice .
Bidirectional Recurrent Neural Networks ( BiRNNs ) are a class of models that overcome this limitation by predicting the outputs as a function of a state variable that accumulates information over the entire input sequence , and by stacking several layers to form a deep architecture able to extract more structure from the input features .<q>Dynamic models go a step further by explicitly incorporating a model of state sequence , but even then , many practical implementations are limited to a low-order finite-state machine .<q>In this work we examine a new application of BiRNNs to the task of classifying categorical prosodic events , and demonstrate that they outperform baseline systems .
Speaker anti-spoofing is crucial to prevent security breaches when the speaker verification systems encounter the spoofed attacks from the advanced speech synthesis algorithms and high fidelity replay devices .<q>Our proposed system yielded an EER of 9.01 % on ASVspoof 2017 , while the best single system and the average scores fusion obtained the evaluation EER of 2.39 % and 0.96 % on ASVspoof 2019 PA , respectively .<q>In this paper , we propose a framework based on multiple features integration and multi-task learning ( MFMT ) for improving anti-spoofing performance .
The paper presents an empirical model of emphatic word detection , as an alternative to conventional machine-learning-based methods .<q>The model is based on the Probabilistic Amplitude Demodulation ( PAD ) that is iteratively applied for getting syllable and stress modulations , i.e. , using the cascaded PAD method .<q>Speaker-dependent cascaded demodulation , considering specific speaking rate of the speakers , yields to detection accuracy of 86 % -91 % .
Statistical language models ( LM ) play a key role in Automatic Speech Recognition ( ASR ) systems used by conversational agents .<q>We obtain a relative WER reduction of 3 % with a 1-pass decoding strategy and 6 % in a 2-pass decoding framework , over an unadapted model .<q>We show that this framework for contextual adaptation provides accuracy improvements under different possible mixture LM partitions that are relevant for both ( 1 ) Goal-oriented conversational agents where it ’ s natural to partition the data by the requested application and for ( 2 ) Non-goal oriented conversational agents where the data can be partitioned using topic labels that come from predictions of a topic classifier .
This paper proposes a channel selection approach for selecting reliable channels based on selection criterion operating in the short-term modulation spectrum domain .<q>The proposed approach quantifies the relative strength of speech from each microphone and speech obtained from beamforming modulations .<q>Overall improvement in recognition rate is observed using delay-sum and superdirective beamformers compared to the case when the channel is selected randomly using circular microphone arrays .
Multimodal fusion of audio , vision , and text has demonstrated significant benefits in advancing the performance of several tasks , including machine translation , video captioning , and video summarization .<q>Audio-Visual Scene-aware Dialog ( AVSD ) is a new and more challenging task , proposed recently , that focuses on generating sentence responses to questions that are asked in a dialog about video content .<q>To this end , we propose a novel AVSD system using student-teacher learning , in which a student network is ( jointly ) trained to mimic the teacher ’ s responses .
Our proposal makes it possible to exploit the benefits of statistical approaches to build rule-based systems .<q>For example , when developers require deterministic system responses to keep total control on the decisions made by the system , or because the infrastructure employed is designed for rule-based systems using technologies currently used in commercial platforms .<q>In this paper , we propose the use of evolutionary algorithms to automatically obtain the dialog rules that are implicit in a dialog corpus .
Although these models allow for better characterization of score distributions , they still require the target and non-target distributions to be reciprocally symmetric .<q>Recently , we have proposed non-Gaussian linear calibration models able to overcome the limitations of Gaussian approaches .<q>Provided that the score distribution assumptions are sufficiently accurate , generative approaches not only have similar or better performance with respect to LogReg , but also allow for unsupervised or semi-supervised training .
The proposed DNN setup uses a time domain approach of learning filters within the network .<q>We achieve 16.48 % relative error rate improvement in IEMOCAP categorical problem ( with a final weighted accuracy of 75.03 % ) using phone posteriors compared to DNN setup which uses only learned time domain features for temporal context modeling .<q>We propose to exploit phone posteriors as an additional feature in Deep Neural Network ( DNN ) to recognize emotions from raw speech waveform .
We present an extended technique for spoken content retrieval ( SCR ) that exploits the prosodic characteristics of spoken terms in order to improve retrieval effectiveness .<q>For an unseen query , we perform SCR by using an extension of the Okapi BM25 function of probabilistic retrieval that incorporates the prosodic classifier 's predictions into the computation of term weights .<q>Our method promotes the rank of speech segments containing a high number of prosodically prominent terms .
Foreground speech segmentation is the use of features to segment between foreground speech and background interfering sources like noise .<q>The mel frequency cepstral coefficients are obtained from the background segments of both the indoor and outdoor audio samples and are used to train the Support Vector Machine ( SVM ) classifier .<q>The background segments are then considered for determining whether a particular segment is an indoor or outdoor audio sample .
We found that only 16 % of the variability in word-level intelligibility can be explained by the presence of obvious mispronunciations .<q>We compare which words in non-native English speech are likely to be misrecognized and which words are likely to be marked as pronunciation errors .<q>This paper investigates the connection between intelligibility and pronunciation accuracy .
A Bilingual Multimodal Speech Communication Corpus incorporating acoustic data as well as visual data related to face , hands and arms gestures during speech , is presented in this paper .<q>Statistics regarding the number and gender of the speakers , number of words , number of sentences and duration of the recording sessions , are also provided .<q>The corpus has been compiled in two different languages , viz. , French and Spanish .
The irregular-to-regular transformation is performed by analyzing and resynthesizing the residual .<q>The linear prediction residual of irregular sections of speech is replaced by overlap-added frames from a codebook of pitch-synchronous residuals .<q>A recent continuous pitch estimation algorithm is used for interpolating F0 in regions of irregular voice .
This paper explored the application of Bidirectional Long Short-Term Memory ( BLSTM ) , which had the capability of modeling time series , to Mandarin tone recognition to handle the tone variations in continuous speech .<q>In addition , we introduced attention mechanism to guide the model to select the suitable context information .<q>Automatic tone recognition is useful for Mandarin spoken language processing .
This paper investigates the influence of text or phone content on recognition performance .<q>This work is performed using the shortest duration subset of the standard RSR2015 database .<q>With a thorough statistical analysis , the work shows how significant reductions in error rates can be achieved by preventing the use of weak passwords and that improvements in performance are consistent across disjoint speaker subsets .
Although the best decision criterion should be based directly on the evaluation metric , the need for a clean reference makes it impractical for employment .<q>In this paper , we propose a novel specialized speech enhancement model selection ( SSEMS ) approach that applies a non-intrusive quality estimation model , termed Quality-Net , to solve this problem .<q>Experimental results first confirm the effectiveness of the proposed SSEMS approach .
In this work , we propose a new method to learn attention mechanism for domain classification .<q>Attention-based bidirectional long short-term network ( BiLSTM ) models have recently shown promising results in text classification tasks .<q>Unlike the past attention mechanisms only guided by domain tags of training data , we explore using the latent topics in the data set to learn topic attention , and employ it for BiLSTM .
Some of these studies have found that bottleneck features extracted from deep neural networks ( DNNs ) , sometimes called “ deep bottleneck features ” ( DBNFs ) , can reduce the word error rates of ASR systems .<q>In this paper , we propose a method of integrating DBNFs using multi-stream HMMs in order to improve the performance of AVSRs under both clean and noisy conditions .<q>Recent interest in “ deep learning ” , which can be defined as the use of algorithms to model high-level abstractions in data , using models composed of multiple non-linear transformations , has resulted in an increase in the number of studies investigating the use of deep learning with automatic speech recognition ( ASR ) systems .
We show that double joint Bayesian outperforms conventional method on modeling DNN local ( digit-dependent ) i-vectors for speaker verification with random prompted digit strings .<q>It was recently applied to voice pass phrase verification , result in better results on text dependent speaker verification task .<q>Double joint Bayesian is a recently introduced analysis method that models and explores multiple information explicitly from the samples to improve the verification performance .
Fricatives devoice more than stops , and posterior fricatives devoice more than anterior ones .<q>Three large corpora with automatic segmentations produced by forced alignment are used : ESTER , ETAPE and NCCFr .<q>This study investigates the tendency towards word-final devoicing of voiced obstruents in Standard French , and how devoicing is influenced by domain , speech style , manner and place of articulation .
End-to-end neural network systems for automatic speech recognition ( ASR ) are trained from acoustic features to text transcriptions .<q>In this paper , we analyze the learned internal representations in an end-to-end ASR model .<q>We evaluate the representation quality in terms of several classification tasks , comparing phonemes and graphemes , as well as different articulatory features .
We learn to listen and write characters with a joint Connectionist Temporal Classification ( CTC ) and attention-based encoder-decoder network .<q>We present a state-of-the-art end-to-end Automatic Speech Recognition ( ASR ) model .<q>During the beam search process , we combine the CTC predictions , the attention-based decoder predictions and a separately trained LSTM language model .
The deep bottleneck ( BN ) feature based ivector solution has been recognized as a popular pipeline for language identification ( LID ) recently .<q>Experiment results show that systems built on top of tandem deep features obtain 19 % and 42 % relative equal error rate reduction on average on NIST LRE 2007 over the counterpart built on traditional deep BN features and the cepstral feature based LID system , respectively .<q>Then , DNNs are trained both separately and jointly on multilingual corpuses to produce different BN features .
This paper documents the significant components of a state-of-the-art language-independent query-by-example spoken term detection system designed for the Query by Example Search on Speech Task ( QUESST ) in MediaEval 2015 .<q>We developed exact and partial matching DTW systems , and WFST based symbolic search systems to handle different types of search queries .<q>To handle the noisy and reverberant speech in the task , we trained tokenizers using data augmented with different noise and reverberation conditions .
This website contains interactive tools for community-augmented meta-analyses , power analyses , and experimental planning .<q>MetaLab is a growing database of meta-analyses , shared in a github repository and via an interactive website .<q>We expect MetaLab data to be particularly useful to researchers interested in early speech perception .
Keyword Spotting ( KWS ) systems allow detecting a set of spoken ( pre-defined ) keywords .<q>In this paper , we propose the implementation of an open-vocabulary ASR-free KWS system based on speech and text encoders that allow matching the computed embeddings in order to spot whether a keyword has been uttered .<q>Besides , KWS systems could use also word classification algorithms that do not allow easily changing the set of words to be recognized , as the classes have to be defined a priori , even before training the system .
This paper is a post-evaluation analysis of our efforts in VOiCES 2019 Speaker Recognition challenge .<q>EER on the core-core condition of the SITW 2016 challenge dropped from 5.85 % to 1.65 % for system fusions submitted for SITW 2016 and VOiCES 2019 , respectively .<q>In our submission to open condition , we used three x-vector systems and also one system based on i-vectors .
Also , we show that the combined model trained with joint acoustic-residual deep features and the model trained with low pass filtered speech significantly increases the detection accuracy .<q>Speech signal or its transformed linear prediction residual ( LPR ) is the most popular signal representations for GCI detection .<q>Glottal closure instants ( GCI ) also called as instants of significant excitation occur during abrupt closure of vocal folds is a well-studied problem for its many potential applications in speech processing .
In this follow-up investigation , we analyze gamma using t-SNE , a dimension reduction technique to allow visualizing gamma in low dimensional space , at two levels : inter-speaker and intra-speaker .<q>Because the calibration is made with the subject ’ s mouth closed , so the measurement field during calibration is loaded solely by the impedance of the radiation field as seen at the subject ’ s lips and baffled by the subject ’ s face ( geometrical information ) .<q>Examining the same gamma dataset from [ 1 ] , t-SNE yielded good spatial clustering in identifying the 6 different speakers with an accuracy exceeding 90 % , attributable to the inter-speaker variation .
The dynamical system controls a speaker-specific vocal tract model derived by factor analysis of mid-sagittal real-time MRI data and provides input to the articulatory synthesizer , which simulates the propagation of sound waves in the vocal tract .<q>Synthesized vowel-consonant-vowel examples demonstrate the feasibility of the method .<q>This paper presents an initial architecture for articulatory synthesis which combines a dynamical system for the control of vocal tract shaping with a novel MATLAB implementation of an articulatory synthesizer .
While most intelligibility measures are intrusive , i.e. , they require a clean reference signal , this is rarely available in real-time applications .<q>This reference signal is then used as input to STOI .<q>This paper proposes two non-intrusive intelligibility measures , which allow using the intrusive short-time objective intelligibility ( STOI ) measure without requiring access to the clean signal .
This acoustical and Electroglottography ( EGG ) study investigates the effect of coda deletion and co-articulatory phasing on vowels and final coda stops , [ p t k ʔ ] , in Taiwan Min checked tones 3 and 5 syllables .<q>Vowel duration , f0 , spectral tilt ( H1*-A3* ) , cepstral peak prominence ( CPP ) and glottal contact quotient ( CQ_H ) were analyzed .<q>Therefore , these findings suggest that H1*-A3* may play a salient role in checked tone identification , and , as a result , is unaffected by sound change .
OCD is defined as that spacing between the adjacent formants when the level of the spectral valley between them reaches the mean spectral value .<q>To alleviate this difficulty , we propose an objective critical distance ( OCD ) that can be determined from the spectral envelope of a speech signal .<q>It is tedious to conduct perceptual tests to determine the critical distance for various experimental conditions .
In this paper , we address the problem of parameter estimation for the Total Variability Model ( TVM ) [ 1 ] .<q>We show that it is possible to reduce the Maximum Likelihood parameter estimation problem for TVM into a Singular Value Decomposition ( SVD ) problem by making some suitably justified approximations in the likelihood function .<q>In addition , we show that this method is able to increase the efficiency of the ivector extraction procedure , and also lends some interpretability to the extracted ivectors .
In this work , we compare algorithms for the estimation of a known respiratory target signal , measured by respiratory belt transducers positioned across the rib cage and abdomen , from conversational speech .<q>Respiratory diseases such as COPD , asthma , and respiratory infections are common in the elderly population and patients in health care monitoring and medical alert services in general .<q>Evaluation of our model on our database of breathing signal and speech yielded a sensitivity of 91.2 % for breath event detection and a mean absolute error of 1.01 breaths per minute for breathing rate estimation .
Also , with the adversarial gradient of the ASR network , the text-dependency of the speaker embedding vector can be reduced .<q>With the triplet loss , the distances between the embedding vectors of the same speaker are minimized while those of different speakers are maximized .<q>In the experiments , we evaluated our speaker verification framework using the LibriSpeech and CHiME 2013 dataset , and the evaluation results show that our speaker verification framework shows lower equal error rate and better text-independency compared to the other approaches .
Maximum Entropy ( MaxEnt ) language models are powerful models that can incorporate linguistic and non-linguistic contextual signals in a unified framework with a convex loss .<q>MaxEnt models also have the advantage of scaling to large model and training data sizes We present the following two contributions to MaxEnt training : ( 1 ) By leveraging smaller amounts of transcribed data , we demonstrate that a MaxEnt LM trained on various types of corpora can be easily adapted to better match the test distribution of Automatic Speech Recognition ( ASR ) ; ( 2 ) A novel adaptive-training approach that efficiently models multiple types of non-linguistic features in a universal model .<q>Training 10B parameter models utilizing a corpus of up to 1T words , we show large reductions in word error rate from adaptation across multiple languages .
We train statistical machine translation models for each representation and evaluate their outputs on the basis of BLEU-1 scores to determine their efficacy .<q>We consider five low-resource African languages , and we produce three different segmental representations of text data for comparisons against four different segmental representations derived solely from acoustic data for each language .<q>Our experiments produce encouraging results : as we cluster our atomic phonetic representations into more word-like units , the amount information retained generally approaches that of the actual words themselves .
This paper describes our entry to the Voice Conversion Challenge 2016 .<q>Based on the maximum likelihood parameter generation algorithm , the method is a reformulation of the minimum generation error training criterion .<q>It uses a GMM for soft classification , a Mel-cepstral vocoder for acoustic analysis and an improved dynamic time warping procedure for source-target alignment .
We adapted a method classically used in speech and speaker recognition , based on Mel-Frequency Cepstral Coefficients ( MFCC ) extraction and Gaussian Mixture Model ( GMM ) to detect recently diagnosed and pharmacologically treated PD patients .<q>This is a promising result for a potential future telediagnosis of Parkinson ’ s disease .<q>As far as we know , this is the first time that audio recordings from telephone network have been used for early PD detection .
The implications of these findings are relevant for adaptive and incremental conversational systems using expressive speech synthesis and aspiring to communicate the attitude of uncertainty .<q>This work investigates how the vocal effort of the synthetic speech together with added disfluencies affect listeners ’ perception of the degree of uncertainty in an utterance .<q>We introduce a DNN voice built entirely from spontaneous conversational speech data and capable of producing a continuum of vocal efforts , prolongations and filled pauses with a corpus-based method .
A careful observation of the current Czech speech suggests otherwise .<q>However , the late peak is also possible and acceptable : we assume that it might be a signal of contrastive emphasis or an implicational cue .<q>This paper presents a perceptual experiment in which Czech speakers evaluated two contrastive forms of the interrogative melody , specifically the one with a late peak modelled after statements ( as suggested by some authors ) , and the one with an early peak modelled after our empirical data collected previously .
Objective evaluation indicates an average 4 dB improvement in the perceptual SNR of signals using the context-based post-filter , with respect to the noisy signal and an average 2 dB improvement relative to the conventional Wiener filter .<q>In this paper , we propose a postfiltering method to attenuate quantization noise which uses the complex spectral correlations of speech signals .<q>However , retaining performance outside the target bitrate range remains challenging .
The Internet of Things ( IoT ) approach allows wide-covering , powerful AED systems to be distributed across the Internet .<q>Detecting human screaming , shouting , and other verbal manifestations of fear and anger are of great interest to security Audio Event Detection ( AED ) systems .<q>We evaluated the performance of our approximated roughness on the Mandarin Affective Speech corpus and a subset of the Youtube AudioSet for screaming against other low-complexity features .
In this study , we present extensive attention-based networks with data augmentation methods to participate in the INTERSPEECH 2019 ComPareE Challenge , specifically the three Sub-challenges : Styrian Dialect Recognition , Continuous Sleepiness Regression , and Baby Sound Classification .<q>For Baby Sound Sub-challenge , the infant sounds are classified into canonical babbling , non-canonical babbling , crying , laughing and junk/other , and our proposed augmentation framework achieves an UAR of 62.39 % on the test set , which outperforms the baseline by about 3.7 % .<q>For Continuous Sleepiness Sub-challenge , it is defined as a regression task with score range from 1 ( extremely alert ) to 9 ( very sleepy ) .
This paper describes the Speech Technology Center ( STC ) anti-spoofing system submitted for ASVspoof 2017 which is focused on replay attacks detection .<q>The success of Automatic Speaker Verification Spoofing and Countermeasures ( ASVspoof ) Challenge 2015 confirmed the impressive perspective in detection of unforeseen spoofing trials based on speech synthesis and voice conversion techniques .<q>Here we investigate the efficiency of a deep learning approach for solution of the mentioned-above task .
In this work , we first establish the performance of joint modeling of punctuation prediction and disfluency detection using neural networks .<q>Neural networks , however , are relatively under-explored for this task .<q>Inserting proper punctuation marks and deleting speech disfluencies are two of the most essential tasks in spoken language processing .
The results show that integrating this simple step in the end-to-end training pipeline significantly boosts the performance of speaker verification .<q>We conducted experiments on the verification task of the Voxceleb1 dataset .<q>The classical i-vectors and the latest end-to-end deep speaker embeddings are the two representative categories of utterance-level representations in automatic speaker verification systems .
Besides , the raw waveform methods - most of which are based on the time-domain signal - do not significantly outperform the conventional methods .<q>The main distinctions from previous works are a new normalization block and a short-range constraint on the filter weights .<q>In this paper , we propose a frequency-domain feature-learning layer which can allow acoustic model training directly from the waveform .
In this paper , we propose a novel method to generate CNN filter parameters by first sampling from a low-dimensional parameter space and then using a trainable scalar vector to perform a linear combination .<q>It also outperforms a CNN model with a similar number of parameters by a relative improvement of 10.26 % .<q>This filter sampling and combination method ( denoted as FSC ) not only naturally enforces parameter sharing in the low-dimensional sampling space , but also adds to the learning capacity of filters .
Detecting the presence of media sounds in the environment can help avoid such issues .<q>Experiments performed using 378 hours of in-house audio recordings collected by volunteers show an F1 score of 71 % with a recall of 72 % in detecting active media sources .<q>In this work we propose a method for this task based on a parallel CNN-GRU-FC classifier architecture which relies on multi-channel information to discriminate between media and live sources .
Vowel discrimination , in particular , has been studied using a range of tasks , experimental paradigms , and stimuli over the past 40 years , work recently compiled in a meta-analysis .<q>A key research question in early language acquisition concerns the development of infants ’ ability to discriminate sounds , and the factors structuring discrimination abilities .<q>The magnitude of effect sizes analysis revealed order effects consistent with the Natural Referent Vowels framework , with greater effect sizes when the second vowel was more peripheral than the first .
Phrasing structure is one of the most important factors in increasing the naturalness of text-to-speech ( TTS ) systems , in particular for long-form reading .<q>This paper presents how we have built phrasing models based on data extracted from audiobooks .<q>Most existing TTS systems are optimized for isolated short sentences , and completely discard the larger context or structure of the text .
In this paper , we consider single-channel speech enhancement in the short time Fourier transform ( STFT ) domain .<q>Reconstructing the signal using the clean amplitude , the mean squared error is decreased and the PESQ score is increased .<q>We suggest to improve an STFT phase estimate by estimating the initial phases .
In the recent NIST 2016 Speaker Recognition Evaluation ( SRE ) , symmetric score normalization ( S-norm ) and calibration using unlabeled in-domain data were shown to be beneficial .<q>In this work , we benchmark these approaches on several distinctly different databases , after we describe our SRI-CON-UAM team system submission for the NIST 2016 SRE .<q>It is unclear , however , whether those techniques generalize well to other data sources .
The new architecture has three components : an encoding transformer , an attention module and a frame-level senone predictor .<q>Finally the predictor generates the senone posteriors for all speaker sources independently with the knowledge from the context vectors .<q>The experimental results on the artificially mixed two-talker WSJ0 corpus show that our proposed model can reduce the word error rate ( WER ) by more than 15 % relatively compared to our previous PIT-ASR system .
Prototype recognition systems with CD-DNN-HMM [ 3 ] [ 4 ] [ 5 ] acoustic models adapted by fDLR [ 6 ] [ 7 ] [ 8 ] [ 9 ] were implemented and tested for 10 target users .<q>We train Gaussian Mixture Models ( GMMs ) based on the word vector representations [ 1 ] [ 2 ] and develop word clusters and keyword extraction approaches for personalization of the lexicon and language model .<q>It was shown that the personalized lexicon may include much more user-specific words not obtained before , and significant performance improvement in terms of tradeoff relationships between recognition accuracy and real time factor was observed .
However , learning the representation of rare words is a challenging problem causing the NLM to produce unreliable probability estimates .<q>To evaluate the proposed method , we enrich the rare street names in the pre-trained NLM and use it to rescore 100-best hypotheses output from the Singapore English speech recognition system .<q>The proposed method augments the word embedding matrices of pre-trained NLM while keeping other parameters unchanged .
In spite of the great success of the i-vector/PLDA framework , speaker verification in noisy environments remains a challenge .<q>To compensate for the variability of i-vectors caused by different levels of background noise , this paper proposes a new framework , namely SNR-invariant PLDA , for robust speaker verification .<q>Accordingly , an i-vector is represented by a linear combination of three components : speaker , SNR , and channel .
Our experiments demonstrate the potential of our method to train a robot for engaging behaviors in an offline manner .<q>Therefore , we introduce deep Q-network ( DQN ) in a batch reinforcement learning framework , where an optimal policy is learned from a batch data collected using a more controlled policy .<q>A major contribution of this work is the formulation of the problem as a Markov decision process ( MDP ) with states defined by the speech activity of the user and rewards generated by quantified engagement levels .
In order to better facilitate deep learning research in Speech Enhancement , we present a noisy speech dataset ( MS-SNSD ) that can scale to arbitrary sizes depending on the number of speakers , noise types , and Speech to Noise Ratio ( SNR ) levels desired .<q>Recent work shows the efficacy of deep learning for noise suppression , but the datasets have been relatively small compared to those used in other domains ( e.g. , ImageNet ) and the associated evaluations have been more focused .<q>We show that increasing dataset sizes increases noise suppression performance as expected .
A mapping function from acoustic parameters to articulatory parameters is usually developed with a single speaker 's parallel data .<q>Experiments show that both methods can improve the mapping accuracy and that the latter method works better than the former method .<q>Hence the constructed mapping model can work appropriately only for this specific speaker , and applying this model to other speakers degrades the performance of acoustic-to-articulatory mapping .
It is composed of a laptop PC with an open source robot audition system HARK ( Honda Research Institute Japan Audition for Robots with Kyoto University ) and a commercially available low-cost microphone array .<q>In this paper , we briefly introduce our system and show an example analysis of a track recorded at the experimental forest of Nagoya University , in central Japan .<q>HARKBird helps us annotate bird songs and grasp the soundscape around the microphone array by providing the direction of arrival ( DOA ) of each localized source and its separated sound automatically .
The investigation is carried out on the problem of diarization .<q>We propose a technique to utilize the information from the CNN for the weighting of the acoustic data in a segment to refine the statistics accumulation process .<q>In our system , the output of the CNN is a probability value of a speaker change in a conversation for a given time segment .
This paper presents a single-channel speech separation method implemented with a deep recurrent neural network ( DRNN ) using recurrent temporal restricted Boltzmann machines ( RTRBM ) .<q>The proposed method has been evaluated on the IEEE corpus and TIMIT dataset for speech denoising task .<q>In order to alleviate this issue , one RTRBM is employed for modelling the acoustic features of input ( mixture ) signal and two RTRBMs are trained for the two training targets ( source signals ) .
Populations of a bird species can evolve over time to become a new species .<q>Given the success of speech processing methods applied to bird species classification , this paper presents work on developing a measure of bird call similarity .<q>The method is inspired by human speech dialect separation measurement using a representation of the pitch contour micro-structure .
Uncertainty decoding is a framework that utilizes this knowledge of uncertainty in the input features during acoustic model scoring .<q>In this paper , we study the propagation of observation uncertainties through the layers of a DNN-based acoustic model .<q>Wemay also have some measures of uncertainty or variance of the estimate .
In contrast , semi-supervised training does not require matching text data , instead generating a hypothesis using a background language model .<q>We demonstrate that this combined approach reduces the expected error rates over the lattices , and reduces the word error rate ( WER ) on a broadcast task .<q>State-of-the-art semi-supervised training uses lattice-based supervision with the lattice-free MMI ( LF-MMI ) objective function .
This paper therefore evaluates the impact of speaker , language and accent variability on the performance of an acoustic-to-articulatory speech inversion system .<q>The articulatory dataset used in this study consists of 21 Dutch speakers reading Dutch and English words and sentences , and 22 UK English speakers reading English words and sentences .<q>While there are several potential applications of such a method in speech therapy and pronunciation training , performance of such acoustic-to-articulatory inversion systems is not very high due to limited availability of simultaneous acoustic and articulatory data , substantial speaker variability , and variable methods of data collection .
Combing local classification with meta-classifiers at a late fusion decision level we obtained state-of-the-art classification performance .<q>Experiments were carried out on manually corrected transcriptions and on potentially erroneous ASR output .<q>It is also demonstrated that layered hierarchical and cascade training procedures result in better classification performance than the single-layered approach based on a joint classification predicting complex class labels .
In order to model both models ’ structural and parametric uncertainty , a novel form of DNN architecture using non-parametric activation functions based on Gaussian process ( GP ) , Gaussian process neural networks ( GPNN ) , is proposed in this paper .<q>One important issue associated with DNNs and artificial neural networks in general is the selection of suitable model structures , for example , the form of hidden node activation functions to use .<q>Initial experiments conducted on the ARPA Resource Management task suggest that the proposed GPNN acoustic models outperformed the baseline sigmoid activation based DNN by 3.40 % to 24.25 % relatively in terms of word error rate .
This paper describes a conditional neural network architecture for Mandarin Chinese polyphone disambiguation .<q>One goal of polyphone disambiguation is to address the homograph problem existing in the front-end processing of Mandarin Chinese text-to-speech system .<q>The system is composed of a bidirectional recurrent neural network component acting as a sentence encoder to accumulate the context correlations , followed by a prediction network that maps the polyphonic character embeddings along with the conditions to corresponding pronunciations .
This room simulation system has been employed in training acoustic models including the ones for the recently released Google Home .<q>We describe the structure and application of an acoustic room simulator to generate large-scale simulated data for training deep neural networks for far-field speech recognition .<q>Results show that the simulator-driven approach is quite effective in obtaining large improvements not only in simulated test conditions , but also in real / rerecorded conditions .
This work aims to establish an open-source biosignals corpus for investigations on how humans plan and execute interactions with the aim of facilitating robotic mastery of everyday activities .<q>In my talk I will present ongoing research at the Cognitive Systems Lab ( CSL ) , where we explore interaction-related biosignals with the goal of advancing machine-mediated human communication and human-machine interaction .<q>Human interaction is a complex process involving modalities such as speech , gestures , motion , and brain activities emitting a wide range of biosignals , which can be captured by a broad panoply of sensors .
This paper explores the mapping of time and frequency domain aspects of the voice source , focussing on the low end of the source spectrum .<q>However , for Oq values of up to about 0.6 , it maps closely to H1-H2 : beyond this point , H1-H2 reflects a more complex interaction of open quotient and glottal skew .<q>As in the earlier study , a series of glottal pulses were generated , keeping peak glottal flow constant , while systematically varying Oq and Rk .
At the back-end we evaluate effective a two-stage model that exploits a Convolutional Neural Network for pre-trained feature extraction , followed by Deep Neural Network classifiers as a post-trained feature adaptation model and classifier .<q>Acoustic scene classification ( ASC ) using front-end time-frequency features and back-end neural network classifiers has demonstrated good performance in recent years .<q>We assess performance on the DCASE2016 dataset , demonstrating good classification accuracies exceeding 90 % , significantly outperforming the DCASE2016 baseline and highly competitive compared to state-of-the-art systems .
In the speech synthesis systems , the phrase break ( PB ) prediction is the first and most important step .<q>However this method is not fully applicable to Mongolian language , because its word embeddings are inadequate trained , owing to the lack of resources .<q>In this paper , we introduce a bidirectional Long Short Term Memory ( BiLSTM ) model which combined word embeddings with syllable and morphological embedding representations to provide richer and multi-view information which leverages the agglutinative property .
This study is the first to evaluate ASR systems ’ responses to speech from patients at different stages of PD in Spanish .<q>Parkinson ’ s Disease ( PD ) affects motor capabilities of patients , who in some cases need to use human-computer assistive technologies to regain independence .<q>The objective of this work is to study in detail the differences in error patterns from state-of-the-art Automatic Speech Recognition ( ASR ) systems on speech from people with and without PD .
The proposed method is language-independent and robust to noisy speech .<q>Our experiment results show that the extracted intonation contour can match F0 contour by conventional approach in very high accuracy and the estimated long-term pitch movements demonstrate regular characteristics of intonation across languages .<q>This paper presents a method to extract dynamic prosodic structure from speech signal using zero-frequency resonator to detect glottal cycle epoch and filter both voice amplitude and fundamental frequency ( F0 ) contours .
Recurrent neural network language models ( RNNLMs ) can be augmented with auxiliary features , which can provide an extra modality on top of the words .<q>It has been found that RNNLMs perform best when trained on a large corpus of generic text and then fine-tuned on text corresponding to the sub-domain for which it is to be applied .<q>However , in many cases the auxiliary features are available for the sub-domain text but not for the generic text .
Voice-based systems are an essential approach for engaging directly with low-literate and underrepresented populations .<q>Our work will allow other researchers and practitioners to quickly develop high-quality small-vocabulary speech-based applications for under-resourced languages<q>Previous work has taken advantage of high-resource speech recognition technology for low-resource language speech recognition through cross-language phoneme mapping .
We use a WX-based common pronunciation scheme for both languages being mixed and unify the homophones during training , which results in a lower word error rate for systems built using this data .<q>Borrowing occurs when a word from a foreign language becomes part of the vocabulary of a language .<q>In this work , we automatically identify and disambiguate homophones in code-switched data to improve recognition of code-switched speech .
In this paper , we first introduce the background and the details of the proposed method .<q>To resolve this issue , speech enhancement has been intensively studied .<q>The proposed deep neural network ( DNN ) -based framework is motivated by the recent success in the text-to-speech ( TTS ) research in employing DNN as well as high audible-quality output signal of the corpus-based speech enhancement which borrows knowledge from the TTS research field .
Several English datasets have been constructed for this task .<q>Identifying speakers in novels aims at determining who says a quote in a given context by text analysis .<q>This task is important for speech synthesis systems to assign appropriate voices to the quotes when producing audio books .
Designed originally to benchmark spoken dialog systems , it still represents the most well-known corpus for benchmarking Spoken Language Understanding ( SLU ) systems .<q>We propose in this paper to investigate these results obtained on ATIS from a qualitative point of view rather than just a quantitative point of view and answer the two following questions : what kind of qualitative improvement brought DNN models to SLU on the ATIS corpus ?<q>discussed the relevance of this corpus after more than 10 years of research on statistical models for performing SLU tasks .
Conventional FAC methods borrow excitation information ( F0 and aperiodicity ; produced by a conventional vocoder ) from a reference ( i.e. , native ) utterance during synthesis time .<q>We present a framework for FAC that eliminates the need for conventional vocoders ( e.g. , STRAIGHT , World ) and therefore the need to use the native speaker ’ s excitation .<q>Our approach uses an acoustic model trained on a native speech corpus to extract speaker-independent phonetic posteriorgrams ( PPGs ) , and then train a speech synthesizer to map PPGs from the non-native speaker into the corresponding spectral features , which in turn are converted into the audio waveform using a high-quality neural vocoder .
Speech separation can be formulated as a supervised learning problem where a machine is trained to cast the acoustic features of the noisy speech to a time-frequency mask , or the spectrum of the clean speech .<q>Experimental results indicate that the proposed multi-target DNN based method outperforms the DNN based algorithm which optimizes a single target .<q>We show that as expected the mask and speech spectrum based targets yield partly complementary estimates , and the separation performance can be improved by merging these estimates .
This Study investigates how different prosodic features affect native speakers ’ perception of L2 English spoken by Chinese students through dubbing , or re-voicing practice on video clips .<q>With the practice of dubbing , prosodic features , especially timing , can be considerably improved and thus the naturalness of the reproduced utterances increases .<q>Learning oral foreign language through dubbing on movie or animation clips has become very popular in China .
Our proposed two-pass model achieves a 17 % –22 % relative reduction in WER compared to RNN-T alone and increases latency by a small fraction over RNN-T .<q>Recently , a streaming recurrent neural network transducer ( RNN-T ) end-to-end ( E2E ) model has shown to be a good candidate for on-device speech recognition , with improved WER and latency metrics compared to conventional on-device models [ 1 ] .<q>However , this model still lags behind a large state-of-the-art conventional model in quality [ 2 ] .
In order to mitigate this problem , our key idea is to introduce various SLU models that are developed for spoken dialogue systems into the CCLMs .<q>However , it is hard to optimize the CCLMs so as to fully exploit the long-range interactive contexts because conversation-level training datasets are often limited .<q>In this paper , we integrate fully neural network based conversation-context language models ( CCLMs ) that are suitable for handling multi-turn conversational automatic speech recognition ( ASR ) tasks , with multiple neural spoken language understanding ( SLU ) models .
In this paper , we propose a system called the Bubble Cooperative Network ( BCN ) , which aims to predict important areas of individual utterances directly from clean speech .<q>Predicting the intelligibility of noisy recordings is difficult and most current algorithms treat all speech energy as equally important to intelligibility .<q>The masks predicted by a single BCN on several utterances show patterns that are similar to analyses derived from human listening tests that analyze each utterance separately , while exhibiting better generalization and less context-dependence than previous approaches .
We show that our proposed system is able to jointly classify and localize words .<q>We also evaluate the system on a keyword spotting task , and show that it can yield similar performance to strong supervised HMM/GMM baseline .<q>For each sentence , it is trained using as supervision only some of the words ( most frequent ) that are present in a given sentence , without knowing their order nor quantity .
This paper investigates the effect such cues have on speech intelligibility in noise and evaluates their interaction with the established effect of situation-specific semantic cues .<q>Interestingly , whilst increasing subjective intelligibility of the target word , the presence of acoustic cues degraded the objective intelligibility of the speech-based semantic cues by 47.0 % ( equivalent to reducing the speech level by 4.5 dB ) .<q>In everyday life , speech is often accompanied by a situation-specific acoustic cue ; a hungry bark as you ask ‘ Has anyone fed the dog ? ’ .
Prominence perception has been known to correlate with a complex interplay of the acoustic features of energy , fundamental frequency , spectral tilt , and duration .<q>We investigate the complex features learned at different layers of the network as well as the 10-dimensional bottleneck features ( BNFs ) , for the standard acoustic prosodic correlates of prominence separately and in combination .<q>This work focuses on examining the acoustic prosodic representations that binary prominence classification neural networks and autoencoders learn for prominence .
Three segment-level phonetic features of German that are well-known to vary across native speakers were examined .<q>This is presumably a result of the partly insufficient perceptibility of this target feature in the synthetic stimuli and demonstrates the necessity of gaining fine-grained control over the synthesis output , should it be intended to implement capabilities of phonetic convergence on the segmental level in spoken dialogue systems .<q>Still the effect was strongest for the natural stimuli , followed by the HMM stimuli and weakest for the diphone stimuli .
In this paper , we focus on exploring contextual information using DNN .<q>Our experimental comparison with DNN based speech separation in difficult noise scenarios demonstrates the effectiveness of the proposed method in terms of both prediction accuracy and objective speech intelligibility .<q>The bDNN classifier first generates multiple base predictions for a frame from a given window that is centered on the frame and contains multiple neighboring frames , and then aggregates the base predictions for the final prediction .
It is also difficult to record large amounts of high quality audio-visual data for developing audio-visual speech recognition ( AVSR ) systems .<q>To address these issues , a novel Bayesian gated neural network ( BGNN ) based AVSR approach is proposed .<q>Speaker level Bayesian gated control of contributions from visual features allows a more robust fusion of audio and video modality .
Sound event detection is an extension of the static auditory classification task into continuous environments , where performance depends jointly upon the detection of overlapping events and their correct classification .<q>The early detection capability of the system is thus evaluated for the classification of partial events .<q>This paper takes the latter approach , by combining a proven CNN classifier acting on spectrogram image features , with time-frequency shaped energy detection that identifies seed regions within the spectrogram that are characteristic of auditory energy events .
The bidirectional and attention-based LSTMs yield significantly better performance in this task .<q>The different roles of client and agent are modeled by switching between role-dependent layers .<q>Furthermore , each speaker may play a different role in the conversation , such as agent versus client , and thus features related to these roles may be important to the context .
We present a computational model of speech motor control that integrates vocal tract state prediction with sensory feedback .<q>This hierarchical model , called FACTS , incorporates both a high-level and low-level controller .<q>The output of the high-level controller is passed to a low-level controller that can issue motor commands at the level of the speech articulators in order to accomplish the desired constrictions .
These are applied to the task of acoustic modelling for speech synthesis ; objective and subjective results indicate that these representations are useful for the generation of acoustic parameters in a text-to-speech ( TTS ) system .<q>This paper presents a simple count-based approach to learning word vector representations by leveraging statistics of co-occurrences between text and speech .<q>words , syllables ) aligned with a sequence of discrete acoustic events .
On top of that , the frequency response and radiation characteristics of the face mask — depending on the material and shape of the mask — adds to the complexity of analyzing speech under face mask .<q>The measurement setup follows the recording configuration of a speech corpus used for speaker recognition experiments .<q>Wearing a face mask affects the speech production .
This paper proposes a fast learning framework for non-parallel many-to-many voice conversion with residual Star Generative Adversarial Networks ( StarGAN ) .<q>Such shortcut connections accelerate the learning process of the network with no increase of parameters and computational complexity .<q>The residual mapping is realized by using identity shortcut connections from the input to the output of the generator in Res-StarGAN-VC .
In this work , we propose and investigate the use of bottleneck feature networks to normalize differences between whispered and neutral speech modes .<q>Furthermore , we report a substantial reduction in word error rate for cross-mode speech recognition , effectively demonstrate that it is possible to train acoustic models capable of classifying both types of speech without needing any additional whispered speech .<q>Our extensive experiments show that this type of speech variability can be effectively normalized .
In this paper , we propose a new approach for unparallel VC .<q>This paper focuses on using voice conversion ( VC ) to improve the speech intelligibility of surgical patients who have had parts of their articulators removed .<q>To our knowledge , this is the first end-to-end GAN-based unsupervised VC model applied to impaired speech .
Multiple instance learning ( MIL ) is a popular framework for learning from weak labeling .<q>Many sequence learning tasks require the localization of certain events in sequences .<q>In this paper , we compare the `` max '' and `` noisy-or '' pooling functions on a speech recognition task and a sound event detection task .
The bilingual acoustic model is trained using a large Mandarin-English corpus with CTC and sMBR criteria .<q>In this work , we jointly study multilingual and code-switching problems , and present a language-universal bilingual system for Mandarin-English speech recognition .<q>We find that this model , which is not given any information about language identity , can achieve comparable performance in monolingual Mandarin and English test sets compared to the well-trained language-specific Mandarin and English ASR systems , respectively .
In this paper , we study four different methods for GPU acceleration of CNNs : a native implementation using cuBLAS , two implementations based on NVIDIA 's recently released deep-learning cuDNN library , and an implementation based on cuFFT .<q>The paper concludes with results on the end-to-end training speed of our CNN network on an LVCSR task .<q>Despite their widespread use , training large and complex architectures remains very time consuming .
Robustness of distant speech recognition in adverse acoustic conditions , on the other hand , remains a crucial open issue for future applications of human-machine interaction .<q>In this paper , we revise this classical approach in the context of modern DNN-HMM systems , and propose the adoption of three methods , namely , asymmetric context windowing , close-talk based supervision , and close-talk based pre-training .<q>One of the most effective approaches to derive a robust acoustic modeling is based on using contaminated speech , which proved helpful in reducing the acoustic mismatch between training and testing conditions .
The IBM Virtual Voice Creator ( IVVC ) is an end-to-end cloud-based solution for TTS voice customization and voiceover generation in games and animated movies .<q>IVVC also allows the user to control emotional style and emphasis in the synthesized speech .<q>The solution is based on the IBM expressive TTS technology with built-in online voice transformation capabilities .
For speech , these methods suffer from two main problems : under-suppression of noise and over-suppression of target speech .<q>In order to make such a system scalable , we propose here learning a similarity metric using two separate networks , one network processing the clean segments offline and another processing the noisy segments at run time .<q>This system incorporates a ranking loss to optimize for the retrieval of appropriate clean speech segments .
G.107 for narrowband and in ITU-T Rec .<q>These models predict the overall quality experienced by a communication partner on the basis of parameters describing the elements of the transmission channel and the terminal equipment .<q>Predictions are compared to the results of listening-only and conversational tests as well as to signal-based predictions , showing a reasonable prediction accuracy .
In this paper , we test whether the perception of filled-pause ( FP ) frequency and public-speaking performance are mediated by the phonetic characteristics of FPs .<q>Results show strong inter-speaker differences in how and how often FPs are realized .<q>In particular , total duration , vowel-formant pattern ( if present ) , and nasal-segment proportion of FPs were correlated with perceptual data of 29 German listeners who rated excerpts of business presentations given by 68 German-speaking managers .
Convolutional Neural Networks ( CNNs ) are effective models for reducing spectral variations and modeling spectral correlations in acoustic features for automatic speech recognition ( ASR ) .<q>Hybrid speech recognition systems incorporating CNNs with Hidden Markov Models/Gaussian Mixture Models ( HMMs/GMMs ) have achieved the state-of-the-art in various benchmarks .<q>In this paper , inspired by the advantages of both CNNs and the CTC approach , we propose an end-to-end speech framework for sequence labeling , by combining hierarchical CNNs with CTC directly without recurrent connections .
The results suggest that models of speaker size perception will need to include a listener specific parameter for the effect of spectral slope .<q>The results showed that voiced speech with high-frequency enhancement was perceived to arise from smaller speakers .<q>One study with unvoiced speech showed that lifting the slope of the speech spectrum by 6 dB/octave also led to a reduction in the perceived size of the speaker .
To learn the feature mapping , a modified Inception Net with residual block is proposed to optimize the triplet loss function .<q>Experiments show that our system consistently outperforms a conventional i-vector system on short duration speaker verification tasks .<q>In general , to get a robust i-vector representation , a satisfying amount of data is needed in the MAP adaptation step , which is hard to meet under short duration constraint .
In light of the recent success of very deep CNNs in image classification , it is of interest to investigate the deep structure of CNNs for speech recognition in detail .<q>Convolutional Neural Networks ( CNNs ) have achieved state-of-the-art performance when embedded in large vocabulary continuous speech recognition ( LVCSR ) systems due to its capability of modeling local correlations and reducing translational variations .<q>In all previous related works for ASR , only up to two convolutional layers are employed .
This study compares the use of these correlates in the production of English lexical stress contrasts by 10 L1 English and 20 L1 Bengali speakers .<q>Results showed that although Bengali speakers used all four acoustic correlates in similar manner like English speakers , but they produced significantly less native like stress patterns .<q>There were also significant differences in formant patterns across speaker groups , such that Bengali speakers produced English like vowel reduction in certain unstressed syllables , but in other cases Bengali speakers have tendency to either not reduce or incorrectly reduce vowels in unstressed syllables .
It would be best to annotate lattices with the risk measure of interest , the exact word error .<q>This approximation becomes particularly problematic with new types of acoustic models that require flexible alignments .<q>However , the algorithm for this uses finite-state automaton determinisation , which has exponential complexity and runs out of memory for large lattices .
We discuss various limitations that the proposed systems have and introduce methods to effectively use them to detect OOVs .<q>For automatic OOV recovery , we compare the use of different kinds of phonetic and graphemic sub-word units , that can be synthesized into word outputs .<q>In this paper we present a study on building various deep neural network-based speech recognition systems for automatic caption generation that can deal with out-of-vocabulary ( OOV ) words .
This work studies various topologies of Regression Neural Networks for transforming i-vectors from three different systems so that — with slight loss in the accuracy — they are compatible with the reference system .<q>Unfortunately , such migration is very likely to result in the incompatibility between the new and the original i-vectors and , therefore , in the inability of comparing the two .<q>We present the results on the NIST SRE 2010 telephone condition .
The results showed a clear negative impact of more complex music and the presence of lyrics in background music on spoken-word recognition .<q>The results open a path for future work and suggest that social spaces ( e.g. , restaurants , cafés and bars ) should make careful choices of music to promote conversation .<q>Yet , little is known of how specific properties of music impact speech processing .
The effectiveness of the proposed speech enhancement method is demonstrated by adding six different realistic noises to clean speech signals with various SNRs .<q>Consequently , it is shown that the proposed method outperforms comparative methods in terms of signal-to-distortion ratio ( SDR ) and perceptual evaluation of speech quality ( PESQ ) for all kinds of simulated noise and SNR conditions .<q>Conventional NMF-based methods have used a fixed noise dictionary , which often results in performance degradation when the NMF noise dictionary can not cover noise types that occur in real-life recording .
The current work investigates the timing of turn-taking in question-response sequences in naturalistic conversations in Ruuli , an under-studied Bantu language spoken in Uganda .<q>Turn-taking behavior in conversation is reported to be universal among cultures , although the language-specific means used to accomplish smooth turn-taking are likely to differ .<q>type-conforming ) or negative ( i.e .
This paper investigates the evaluation of ASR in spoken language translation context .<q>More precisely , we propose a simple extension of WER metric in order to penalize differently substitution errors according to their context using word embeddings .<q>Our experiments show that the correlation of the new proposed metric with SLT performance is better than the one of WER .
One approach shown to catalyse robust misperceptions is to embed words in noise .<q>We speculate that the inflectional morphology of Spanish lends itself to more easily recruit single elements from a babble masker into valid word hypotheses .<q>The biggest difference is seen for babble noise , which tends to induce relatively complex confusions in English and simpler confusions in Spanish .
This paper develops a method for assessing the crosslinguistic consistency of phonological features in phoneme inventories .<q>The concept of a phoneme arose historically as a theoretical abstraction that applies language-internally .<q>The method involves training separate binary neural classifiers for several phonological contrast in audio spans centered on particular segments within continuous speech .
Speech Emotion Recognition ( SER ) systems currently are focusing on classifying emotions on each single language .<q>Firstly , to predict multilingual emotion dimensions accurately such as human annotations .<q>In this paper , we therefore present a SER system in a multilingual scenario from perspective of human perceptual processing .
In addition , the results suggested system robustness for unseen speakers when combined with speaker features .<q>We tested the proposed speech-enhancement system on the TIMIT dataset .<q>To increase the system performance over environmental variations , we propose a novel speaker-aware system that integrates a deep denoising autoencoder ( DDAE ) with an embedded speaker identity .
Speech broadcasting via loudspeakers is widely used in public transportation to send broadcast notifications .<q>Such a transmission system is realized with a 20 kHz carrier frequency because it is inaudible to most listeners but capable of being used in communication between a loudspeaker and microphone .<q>In addition , the performance of the proposed ultrasonic communication method is evaluated by measuring the success rate of transmitted words under various signal-to-noise ratio conditions .
Since the strengths of j-vector and joint Bayesian analysis appear complementary the joint learning significantly outperforms traditional separate training scheme .<q>In this paper we present a integrated approach to text dependent speaker verification , which consists of a siamese deep neural network that takes two variable length speech segments and maps them to the likelihood score and speaker/phrase labels , where the likelihood score as a loss guide is computed by a variant joint Bayesian model .<q>However current state-of-the-art framework often consider training the J-vector extractor and the joint Bayesian classifier separately .
A widely used method to decompose a speech signal into the TFS and ENV is the Hilbert transform .<q>A speech signal can be viewed as a high frequency carrier signal containing the temporal fine structure ( TFS ) that is modulated by a low frequency envelope ( ENV ) .<q>Although this method has been available for about one century and is widely applied in various kinds of speech processing tasks ( e.g .
The proposed enhancement framework is applied in synthesized speech and evaluated in presence of different types and levels of noise .<q>A novel method for enhancement of strength of excitation is proposed which makes the synthesized speech more intelligible in practical scenario .<q>Text-to-speech ( TTS ) synthesis systems have grown popularity due to their diverse practical usability .
Finally , an integrated score is obtained by a logistic regression model , which is trained with a large spoken document and automatically generated spoken queries as development data .<q>In this paper , we propose combining feature-based acoustic match which is often employed in the STD systems for low resource languages , along with the other ASR-derived features .<q>Second , DTW-based acoustic match between the query and candidate segment is performed using the posterior features derived from a monophone-state DNN .
Glottal volume velocity waveform , the acoustical excitation of voiced speech , can not be acquired through direct measurements in normal production of continuous speech .<q>Glottal inverse filtering ( GIF ) , however , can be used to estimate the glottal flow from recorded speech signals .<q>In this study , we investigate how the DNN-based generation of glottal pulses is affected by training data variety .
We present a new model for singing synthesis based on a modified version of the WaveNet architecture .<q>Using a simple multi-stream architecture , harmonic , aperiodic and voiced/unvoiced components can all be predicted in a coherent manner .<q>We compare our method to existing parametric statistical and state-of-the-art concatenative methods using quantitative metrics and a listening test .
In this paper , we strive to move the field towards more realistic and challenging scenarios .<q>To that end , we created the WSJ0 Hipster Ambient Mixtures ( WHAM ! )<q>dataset , consisting of two speaker mixtures from the wsj0-2mix dataset combined with real ambient noise samples .
We introduce a method for predicting midsagittal contours of orofacial articulators from real-time MRI data .<q>This work opens new perspectives for studying articulatory motion in speech .<q>A corpus of about 26 minutes of speech has been recorded of a French speaker at a rate of 55 images / s using highly undersampled radial gradient-echo MRI with image reconstruction by nonlinear inversion .
This paper presents an attempt to evaluate three different sets of features extracted from prosodic descriptors and Big Five traits for building an anomaly detector .<q>The Big Five model enables to capture personality information .<q>In this work , we propose the above mentioned unsupervised or semi-supervised methods , and discuss their performance , to detect particular audio-clips produced by a speaker with an abnormal personality .
Speech and language features have been proven to be useful for the detection of neurodegenerative diseases , such as Alzheimer ’ s disease ( AD ) , and its prodromal stage , mild cognitive impairment ( MCI ) .<q>To bridge this gap , the present study aimed to design a speech database of Chinese elderly with intact cognition and MCI , named “ Mandarin Elderly Cognitive Speech Database ” ( MECSD ) .<q>The MECSD will provide researchers with access to a large shared database that can facilitate hypothesis testing in the study of early-stage dementia .
The present study examines the validity of a well-defined set of phonological representations for the generation of intonation in the nuclear region of an intonational phrase in American English : namely , the combination of binary pitch accents ( H*/L* ) , phrase accents ( H-/L- ) , and boundary tones ( H % /L % ) proposed in Pierrehumbert ( 1980 ) .<q>Speakers largely distinguished rising from non-rising contours in production , but few other distinctions were maintained .<q>Understanding the structure of intonational variation is a longstanding issue in prosodic research .
They include stochastic acceptors , which represent certain probability distributions over random strings .<q>Sampling is also effective in the presence of non-injective transformations of strings .<q>We can sample efficiently from the transformed labeling distribution and use this in two different strategies for finding the most probable CTC labeling .
Neural networks , including feed-forward deep neural networks ( DNNs ) and recurrent neural networks ( RNNs ) with long short-term term memory ( LSTM ) cells , are adopted to map EMA features towards not only spectral features ( i.e .<q>power , U/V flag and F0 ) .<q>A cascaded prediction strategy is proposed to utilize the predicted spectral features as auxiliary input to boost the prediction of excitation features .
We propose role play dialogue-aware language models ( RPDA-LMs ) that can leverage interactive contexts in role play multi-turn dialogues for estimating the generative probability of words .<q>We compose the RPDA-LMs by extending hierarchical recurrent encoder-decoder modeling so as to handle the role information .<q>In addition , we verify the effectiveness of explicitly taking interactive contexts into consideration .
Automatic speech recognition ( ASR ) systems are increasingly used to transcribe text for publication or official uses .<q>With re-scoring , the sentence error rate ( SER ) for utterances containing two errors ( and hence with SER=100 % ) drops to 82.77 % and for utterances containing three errors drops to 95.88 % .<q>The research presented in this paper is conducted within an ASR-based transcription system with human post-editing for the Icelandic parliament , Althingi , and aims to automatically correct down-stream errors once the first error of a sentence has been manually corrected .
In this paper , we propose a discriminative model to account for the inherent heterogeneity in the reliability of annotations associated with a sample while training automatic classification models .<q>Many computational paralinguistic tasks need to work with noisy human annotations that are inherently challenging for the human annotator to provide .<q>Comparing against a baseline of reliability-blind maximum entropy model , we show that there is merit to reliability-aware classification when the feature set is unreliable .
In order to simultaneously learn the representation of the original short-utterance i-vectors and fit the target long-version i-vectors , we jointly train a supervised-regression model with an autoencoder using CNNs .<q>We compare our proposed CNN-based joint mapping method with a GMM-based joint modeling method under matched and mismatched PLDA training conditions .<q>I-vector and probabilistic linear discriminant analysis ( PLDA ) based systems have become the standard in speaker verification applications , but they are less effective with short utterances .
The second stage involves the estimation of the air-tissue boundaries from the co-registered EMA points .<q>Experiments with the EMA and rtMRI recordings of five sentences spoken by one male and one female speakers show that the VT shape in the mid-sagittal plane can be recovered from the EMA flesh points with an average reconstruction error of 2.55 mm and 2.75 mm respectively .<q>Electromagnetic articulograph ( EMA ) provides movement data of sensors attached to a few flesh points on different speech articulators including lips , jaw , and tongue while a subject speaks .
Char+CV-CTC achieved the best ASR results with a 2.2 % Character Error Rate and a 6.1 % Word Error Rate ( WER ) on the Eval92 evaluation subset .<q>Experiments were carried out on the Wall Street Journal ( WSJ ) corpus .<q>We compare several MTL architectures that jointly learn to predict characters ( sometimes called graphemes ) and consonant/vowel ( CV ) binary labels .
The task of detecting the vowel regions in a given speech signal is a challenging problem .<q>Exploiting the differences in the evidences obtained by using the two kinds of features , a technique to combine the evidences is also proposed in order to get a better estimate of the VOPs and VEPs .<q>Statistical modeling based on deep neural network has been employed for learning the parameters .
We experiment with two variants of the standard AE which we have named Binarized Autoencoder and Hidden-Markov-Model Encoder .<q>Binarized AEs can outperform standard AEs when using a larger number of encoding nodes , while HMM Encoders may allow more compact subword transcriptions without worsening the ABX performance .<q>Our approach is based on Deep Autoencoders ( AEs ) , whose encoding node values are thresholded to subsequently generate a symbolic , i.e. , 1-of-K ( with K = No .
To identify distinct sound events for each scene , we formulate ASC in a multi-instance learning ( MIL ) framework , where each audio recording is mapped into a bag-of-instances representation .<q>The experiments were conducted on the official development set of the DCASE2018 Task1 Subtask B , and our best-performing model improves over the official baseline by 9.4 % ( 68.3 % vs 58.9 % ) in terms of classification accuracy .<q>Furthermore , we propose two specially designed modules that model the multi-temporal scale and multi-modal natures of the sound events respectively .
This study follows up on our pioneering work in designing a Pneumatic Bionic Voice ( PBV ) prosthesis for larynx amputees .<q>PBV prostheses are electronic adaptations of the traditional Pneumatic Artificial Larynx ( PAL ) device .<q>A Gaussian mixture model of the joint probability density of respiration and PAL voice features is implemented to estimate the excitation waveform of the PBV .
Articulatory phonology provides a formalism to understand coarticulation through spatiotemporal changes in the patterns of underlying gestures .<q>The TV estimators are trained on the University of Wisconsin X-ray Microbeam ( XRMB ) database .<q>This paper studies the coarticulation occurring in certain fast spoken utterances using articulatory constriction tract-variables ( TVs ) estimated from acoustic features .
Deep neural network ( DNN ) acoustic models yield posterior probabilities of senone classes .<q>Evaluation on AMI meeting corpus yields nearly 4 % absolute reduction in word error rate ( WER ) compared to the baseline DNN trained with cross entropy objective .<q>We demonstrate how to combine the gains from eigenposterior based enhancement with sequence discrimination to improve ASR using semi-supervised training .
The results show that , similar to humans , DNN systems learn speaker-adapted phone category boundaries from a few labeled examples .<q>We visualize the activations in the hidden layers of the DNN during perceptual learning .<q>Comparisons between DNNs and humans can thus provide valuable insights into the way humans process speech and improve ASR technology .
As supervised resource , we restrict ourselves to a small , easily acquired and independently compiled set of isolated keywords .<q>We conclude that integrating BNFs with the CAE allows both large out-of-domain and sparse in-domain resources to be exploited for improved ASR-free keyword spotting .<q>Such quickly-deployable systems aim to support United Nations ( UN ) humanitarian relief efforts in parts of Africa with severely under-resourced languages .
For the PPB features we developed a special novel very fast , simple and efficient OOV decoder .<q>The proposed approach is based on using high-level features from an automatic speech recognition ( ASR ) system , so called phoneme posterior based ( PPB ) features , for decoding .<q>These features are obtained by calculating time-dependent phoneme posterior probabilities from word lattices , followed by their smoothing .
Example-based speech enhancement is a promising single-channel approach for coping with highly nonstationary noise .<q>In this paper , instead of using a GMM with noise robust BNFs , we propose the direct use of a DNN-AM in the example search to further improve its noise robustness .<q>Then , it concatenates the clean speech examples that are paired with the matched noisy examples to obtain an estimate of the underlying clean speech component in the input .
We present the first cross-lingual study on the detection of laughter and fillers in conversational and spontaneous speech collected ‘ in the wild ’ over IP ( internet protocol ) .<q>We show that LSTM and GRU architectures are valid alternatives for e. g. , on-line and compute-sensitive applications , since their application incurs a relative UAAUC decrease of only approximately 5 % with respect to our best systems .<q>The automatic detection and classification of social signals is an important task , given the fundamental role nonverbal behavioral cues play in human communication .
We introduce a novel method for stabilizing the normalized scores .<q>We explicitly estimate a low dimensional subspace in supervector space which accounts for high variability in score normalization parameters .<q>We then compensate the estimated subspace .
We propose the mel-scale relative phase feature and apply source-filter vocal tract feature in phase domain for replay attacks detection .<q>In addition to these phase haracteristics , constant Q cepstral coefficients ( CQCCs ) are used .<q>However , they are still affected by different kind of spoofing attacks .
Hypernasality refers to the perception of excessive nasal resonances in vowels and voiced consonants .<q>The posterior probabilities obtained for nasal sentence class are referred to as hypernasality scores .<q>Motivated by the functionality of nasometer , in this work , a method is proposed for the evaluation of hypernasality .
Speech impairments are one of the earliest manifestations in patients with Parkinson ’ s disease .<q>A robust strategy to model the articulatory deficits related to the starting or stopping vibration of the vocal folds is proposed in this study .<q>The proposed approach improves the results previously reported in the literature .
Augmenting the inputs with DR features improves objective acoustic scores on a test set and leads to significant preference by listeners in a forced choice AB test for naturalness .<q>In order to test our hypotheses , we prepare a dataset of DR-annotated transcriptions of audiobooks in English .<q>Then , we use the corpus to train a neural SPSS system in two configurations : a baseline configuration making use only of conventional linguistic features , and an experimental one where these are supplemented with DRs .
This work investigates learning acoustic units in an unsupervised manner from real-world speech data by using a cascade of an autoencoder and a Kohonen net .<q>One major bottleneck for existing speech-processing systems is their reliance on transcriptions .<q>Instead , a global subword unit description , such as a universal phone set , is typically used in such scenarios .
In this study , we propose the use of recently developed Augmented cycle-consistent adversarial networks ( Augmented CycleGANs ) for conversion between normal and Lombard speaking styles .<q>It is hence desirable to have a system capable of converting speech from normal to Lombard style .<q>We utilize a parametric approach that uses the Pulse Model in Log domain ( PML ) vocoder to extract features from normal speech that are then mapped to Lombard-style features using the Augmented CycleGAN .
We present a novel approach to vocal tract airway tissue boundary tracking by training a statistical shape and appearance model for human vocal tract .<q>This segmentation problem is however challenging due to poor resolution of real-time speech MRI , grainy noise and the rapidly varying vocal tract shape .<q>Knowledge about the dynamic shape of the vocal tract is the basis of many speech production applications such as , articulatory analysis , modeling and synthesis .
With the aim to construct another type of DNN-based speech synthesizer with neither the vocoder nor computational explosion , we investigated direct modeling of frequency spectra and waveform generation based on phase recovery .<q>In this framework , STFT spectral amplitudes that include harmonic information derived from F0 are directly predicted through a DNN-based acoustic model and we use Griffin and Lim ’ s approach to recover phase and generate waveforms .<q>The experimental results showed that the proposed system synthesized speech without buzziness and outperformed one generated from a conventional system using the vocoder .
In this paper , we present a novel method for term score estimation .<q>The RNN is trained from recognition results on word- and phone-level in an unsupervised fashion without need of any hand-labeled data .<q>The method is primarily designed for scoring the out-of-vocabulary terms , however it could also estimate scores for in-vocabulary results .
In this paper , we investigated several approaches of infant voice detection and segmentation , including a rule-based voice activity detector , hidden Markov models with Gaussian mixture observation models , support vector machines , and random forests .<q>Automated detection of vocalisations in the very first year of life is still a stepchild of this field .<q>At the same time , our results show that , a fully automated approach for this problem is yet to come .
Using a corpus of multilingual recordings of a standard text ( the North Wind and the Sun passage , NWS ) in 11 languages , speaking rate ( SR , syllables/second ) and information density ( ID , number of syllables for the NWS text ) were examined in first-language ( L1 ) and second-language ( L2 ) speech .<q>These findings suggest that L2 speech production involves temporal restructuring beyond increased segment , syllable and word durations , and that the resultant information rate ( information bits transmitted/second ) of L2 speech diverges substantially from that of L1 speech .<q>That is , L2 English involved more syllables than L1 English for the same NWS text .
We augment a set of synthesized articulatory videos for 50 words obtained from the MRI-TIMIT database .<q>For this , real time magnetic resonance imaging ( rt-MRI ) video image-frames ( IFs ) containing articulatory movements have been used .<q>We , in this work , propose an augmentation method using pixel intensities in the regions enclosed by the articulatory boundaries obtained from air-tissue boundaries ( ATBs ) .
These results suggest that appropriate combinations of speaking styles and alerting sounds will increase the intelligibility of emergency PA announcements .<q>Eighteen young participants carried out word identification test and rated perceived urgency on five-point scales in noisy and reverberant environments .<q>Public-address ( PA ) announcements are used to convey emergency information ; however , noise and reverberation sometimes make announcements in public spaces unintelligible .
Keyword spotting ( KWS ) for low-resource languages has drawn increasing attention in recent years .<q>It has been shown that considerable KWS gains can be obtained by combining the keyword detection results from different forms of ASR systems , e.g. , Tandem and Hybrid systems .<q>Experiments on six Babel OP2 development languages show that joint decoding is capable of providing consistent gains over each individual system .
This paper developed an automatic detection system of prosodic focus in American English , using telephone-number strings .<q>We discuss the predictive features in our model and potential features to add in the future study .<q>We also compared the model performance to human judgment rates from a perception experiment with 67 native speakers of American English .
In this paper , we tightly integrate spectral and spatial information for target speaker extraction .<q>SpeakerBeam is a state-of-the-art method for extracting a speech signal of target speaker from a mixture using an adaption utterance .<q>The existing multi-channel SpeakerBeam utilizes the spectral features of the signals with the ignorance of the spatial discriminability of the multi-channel processing .
Neural networks have a reputation for being `` black boxes '' , which it has been suggested that techniques from user interface development and visualisation in particular , could help lift .<q>By applying LDA to the features of higher dimensional non-bottleneck layers , we observe a triangular pattern which may indicate that silence , friction and voicing are the three main properties learned by the neural networks .<q>The 9-dimensional BNFs obtained from a phone classification neural network are visualised in 2-dimensional spaces using linear discriminant analysis ( LDA ) and t-distributed stochastic neighbour embedding ( t-SNE ) .
Speech separation has been very successful with deep learning techniques .<q>We construct a time-and-frequency feature map by concatenating 1-dim convolution encoded feature map ( for time domain ) and magnitude spectrogram ( for frequency domain ) , which was then processed by an embedding network and clustering approaches very similar to those used in time and frequency domain prior works .<q>Very impressive work achieving speech separation over time domain was reported recently , probably because waveforms in time domain may describe the different realizations of speech in a more precise way than magnitude spectrogram lacking phase information .
Speech recognition in multi-channel environments requires target speaker localization , multi-channel signal enhancement and robust speech recognition .<q>Results indicate that the system attains on real-world evaluation data a relative improvement of 31.98 % over the baseline and of 21.44 % over a modified baseline .<q>The main contribution of the present work is the introduction of a probabilistic approach to ( re- ) estimation of location-specific steering vectors based on weighting of observed inter-channel phase differences with the spatial source probability map derived in the localization step .
Knowledge about the location of obstruction site ( V—Velum , O—Oropharyngeal lateral walls , T—Tongue , E—Epiglottis ) in the upper airways is necessary for proper surgical treatment .<q>In this paper we propose a dual source-filter model similar to the source-filter model of speech to approximate the generation process of snore audio .<q>The second filter models the vocal tract from the obstruction point to the lips/nose with impulse train excitation which represents vibrations at the point of obstruction .
In this paper , we explore speech demodulation-based features using Hilbert transform ( HT ) and Teager Energy Operator ( TEO ) for replay detection .<q>To explore possible complementary information using amplitude and frequency , we have used score-level fusion of IA and IF .<q>Replay attack presents a great threat to Automatic Speaker Verification ( ASV ) system .
This study investigates the influence of prosody transplantation on foreign accent ratings .<q>Syllable duration and pitch contour were transferred from utterances of a male and female German native speaker to utterances of ten French native speakers speaking German .<q>Acoustic measurements show that French learners spoke with a significantly lower speaking rate .
Data augmentation is a common approach to increasing the amount of training data .<q>A reduction in word error rate is demonstrated on four languages from the IARPA Babel program .<q>Low resourced languages suffer from limited training data and resources .
In this paper , we are describing our developed systems for the 2018 SLaTE CALL Shared Task on grammatical and linguistic assessment of English spoken by German-speaking Swiss teenagers .<q>In this work , we focused on the text-processing component .<q>Errors related to the meaning are detected using novel approaches which measure the similarity between the given response and stored set of reference responses .
In this paper , neural attention model is applied on two sequence labeling tasks , dialogue act detection and key term extraction .<q>The experimental results show that with the attention mechanism , discernible improvements were achieved in the sequence labeling task considered here .<q>If the information in the whole sequence is treated equally , the noisy or irrelevant part may degrade the classification performance .
This paper examines Fitts ’ law in speech articulation kinematics by analyzing USC-TIMIT , a large database of real-time magnetic resonance imaging data of speech production .<q>Consonant targets , and particularly those following vowels , show the strongest evidence of this tradeoff , with correlations as high as 0.71 between movement time and difficulty .<q>This paper also addresses methodological challenges in applying Fitts-style analysis , including the definition and operational measurement of key variables in real-time MRI data .
Initial and selected data are then used to build acoustic and language models for speech recognition .<q>The AL selection also outperforms the VLLP baseline for other IARPA-Babel languages , and will be further tested in the upcoming NIST OpenKWS 2015 evaluation .<q>AL methods based on different selection criteria have been explored .
We first obtain the word boundaries of speech and singing vocals with forced alignment using speech and singing adapted acoustic models , respectively .<q>We attempt to align speech to singing vocal using a combination of model-based forced alignment and feature-based dynamic time warping ( DTW ) .<q>In this work , we propose a novel method to align phonetically identical spoken lyric with a singing vocal in a speech-singing parallel corpus , that is needed in speech-to-singing conversion .
In this paper , we propose a novel acoustic-to-articulatory mapping model based on mixture of probabilistic canonical correlation analysis ( mPCCA ) .<q>In PCCA , it is assumed that two different kinds of data are observed as results from different linear transforms of a common latent variable .<q>mPCCA is an expansion of PCCA and it can model a much more complex structure .
Such large amounts of acquired data require scalable automated methods to identify different articulators ( e.g. , tongue , velum ) for further analysis .<q>We pose this as a pixel labeling problem , with the outline contour of each articulator of interest as positive class and the remaining tissue and airway as negative classes .<q>In this paper , we propose a convolutional neural network with an encoder-decoder architecture to jointly detect the relevant air-tissue boundaries as well as to label them , which we refer to as ‘ semantic edge detection ’ .
We have previously proposed to use binaurally recorded signals to estimate the speech intelligibility and compared the estimation accuracy of several machine learning methods on this signal .<q>However since intelligibility tests are commonly conducted using monaurally recorded signals , the intelligibility is often underestimated compared to live human listeners since this segregation capability is neglected .<q>In this paper , we attempt to introduce deep neural networks ( DNN ) to this task .
First , a recurrent neural network ( RNN ) encoder-decoder is trained on the task of predicting nearby turns on the full dialogue corpus ; next , the RNN encoder is reused as a feature representation for the supervised learning problem .<q>While previous work has explored the use of pre-training for non-dialogue corpora , our method is specifically geared toward the dialogue use case .<q>We demonstrate an improvement on a clinical documentation task , particularly in the regime of small amounts of labeled data .
We study the effect of the belief state representation within A2C under 0 % , 15 % , 30 % and 45 % semantic error rate and conclude that the novel binary representation in general outperforms both the domain-independent and the verbose belief state representation .<q>To test those representations , the recently introduced Advantage Actor Critic ( A2C ) algorithm is exploited .<q>The results indicate that the proposed compact , binary representation performs better or similarly to the other representations , being an efficient and promising alternative to the full belief .
Machine learning models for speech-based depression classification offer promise for health care applications .<q>We examine performance as a function of response input length for two NLP systems that differ in overall performance .<q>We analyze results for speaker-independent depression classification using a corpus of over 1400 hours of speech from a human-machine health screening application .
Early , accurate detection of Parkinson 's disease may aid in possible intervention and rehabilitation .<q>We also introduce articulatory features based on a neural computational model of speech production , the Directions into Velocities of Articulators ( DIVA ) model .<q>We introduce acoustic biomarkers reflecting the segment dependence of changes in speech production components , motivated by disturbances in underlying neural motor , articulatory , and prosodic brain centers of speech .
A possible side-effect of exposure to non-native sounds is a change in the way we perceive native sounds .<q>Listeners exhibited a significantly reduced noise tolerance for the Chinese consonants /l/ and /w/ following exposure to Spanish .<q>Some differences between the two sounds in the time-course of recovery from perceptual adaptation were observed .
Windowing centered around unambiguous peaks in AP residual is used for segmentation , followed by pitch/duration modification of AP residual by mapping of pitch markers .<q>The modified speech signal is obtained from modified AP residual using synthesis filters .<q>The phase spectrum is parametrically modeled as the response of an allpass ( AP ) filter , and the filter coefficients are estimated by considering the linear prediction ( LP ) residual as the output of the AP filter .
The model used in this work includes the direct impact of the acoustic pressure on the transversal plane of the vocal folds and an acoustic perturbation component to the glottal flow .<q>Such glides reveal multi-faceted nonlinear system behaviour when the fundamental frequency f_o is near the first vocal tract resonance f_ { R1 } .<q>In simulated glottal flow waveforms , the tendency towards a formant ripple increases when acoustic feedback to glottal flow is enabled , whereas the phenomenon occurs more rarely as a result of the direct acoustic pressure to vocal folds .
In a human-machine dialog scenario , deciding the appropriate time for the machine to take the turn is an open research problem .<q>The re-labeling consists of assigning a binary value to each token in the user utterance that allows to identify the appropriate point for taking the turn .<q>To identify the point of maximal understanding in an ongoing utterance , we a ) implement an incremental Dialog State Tracker which is updated on a token basis ( iDST ) b ) re-label the Dialog State Tracking Challenge 2 ( DSTC2 ) dataset and c ) adapt it to the incremental turn-taking experimental scenario .
adding the score provided by a backward LSTMLM , which uses succeeding words to predict the current word , and employing ensemble encoders , which have a high feature encoding capability .<q>We have proposed a neural network ( NN ) model called a deep duel model ( DDM ) for rescoring N-best speech recognition hypothesis lists .<q>In this study , we further improve the selection performance by introducing two modifications , i.e .
In this paper , we evaluate the accuracy of state-of-the-art automatic speech recognition systems ( ASRs ) on two dysarthric speech datasets and compare the results to ASR performance on control speech .<q>Speech is a complex process that can break in many different ways and lead to a variety of voice disorders .<q>Dysarthria is a voice disorder where individuals are unable to control one or more of the aspects of speech—the articulation , breathing , voicing , or prosody—leading to less intelligible speech .
Listening tests and Mean Opinion Scores ( MOS ) are the most commonly used techniques for the evaluation of speech synthesis quality and naturalness .<q>Bias introduced by individual participants and synthesized text can a dramatic impact on observed MOS scores .<q>For example , we find that on average the mean difference between the highest and lowest scoring rater is over 2 MOS points ( on a 5 point scale ) .
We convert a pretrained WFST to a trainable neural network and adapt the system to target environments/vocabulary by E2E joint training with an AM .<q>By pooling output score sequences , a vocabulary posterior for each utterance is obtained and used for discriminative loss computation .<q>We also adapt each language system to the other language using the adaptation data , and the results show that the proposed method also works well for language adaptations .
The quality of the synthesized speech obtained with the Riesz envelope is compared with that obtained using the envelope estimated by the WORLD vocoder .<q>Unlike the conventional spectrogram , we consider a pitch-adaptive spectrogram and model a spectro-temporal patch using an amplitude- and frequency-modulated two-dimensional ( 2-D ) cosine signal .<q>We employ a demodulation technique based on the Riesz transform that we proposed recently to estimate the amplitude and frequency modulations .
Sequence discriminative training criteria have long been a standard tool in automatic speech recognition for improving the performance of acoustic models over their maximum likelihood / cross entropy trained counterparts .<q>Lattice-based sMBR still outperforms all lattice-free training criteria .<q>We compared performance , speed of convergence , and stability on large vocabulary continuous speech recognition tasks like Switchboard and Quaero .
Speech samples produced by 67 children with FAD and 30 typically developing children .<q>Articulatory-acoustic vowel space features , including formant centralization ratio ( FCR3 ) , F1 range ratio ( F1RR ) , F2 range ratio ( F2RR ) and triangular vowel space area ( TVSA ) , were calculated using the first two formant frequencies from vowels /a/ , /i/ , /u/ .<q>The findings support the notion that these acoustic features can be used to differentiate misarticulated speech from healthy speech and could be used to objectively classify and evaluate FAD speech .
Parameters are trained with dual objectives : one that promotes a selective bandpass filter that eliminates noise at time-frequency positions that exceed signal power and another that proportionally splits time-frequency content between signal and noise .<q>Our embedding space directly optimizes for the discrimination of speaker and noise by jointly modeling their characteristics .<q>Our approach is inspired by the recent success of neural network models separating speakers from other speakers and singers from instrumental accompaniment .
Mel-Frequency Cepstral Coefficients ( MFCCs ) are used as features , and Principle Component Analysis ( PCA ) is used and slightly adjusted to simulate the perceptual space .<q>The language pairs studied are English and Mandarin Chinese and only their vowel inventories are considered .<q>Theories of second language ( L2 ) acquisition of phonology / phonetics / pronunciation / accent often resort to the similarity/ dissimilarity between the first language ( L1 ) and L2 sound inventories .
Punctuation of ASR-produced transcripts has received increasing attention in the recent years ; RNN-based sequence modelling solutions which exploit textual and/or acoustic features show encouraging performance .<q>Subjective tests involve both normal hearing and deaf or hard-of-hearing ( DHH ) subjects .<q>Switching the focus from the technical side , qualifying and quantifying the benefits of such punctuation from end-user perspective have not been performed yet exhaustively .
The posterior probabilities are estimated for phonetic and phonological classes using deep neural network ( DNN ) computational framework .<q>Exploiting the class-specific sparsity leads to a simple quantized posterior hashing procedure to reduce the search space of posterior exemplars .<q>The k nearest neighbor ( kNN ) method is applied for posterior based classification problems .
We present the USC Speech and Vocal Tract Morphology MRI Database , a 17-speaker magnetic resonance imaging database for speech research .<q>We acquired 2D real-time MRI of vocal tract shaping during consonant-vowel-consonant sequences , vowel-consonant-vowel sequences , read passages , and spontaneous speech .<q>We acquired 3D volumetric MRI of the full set of vowels and continuant consonants of American English .
The second English Multi-Genre Broadcast Challenge ( MGB-3 ) is a controlled evaluation of speech recognition and lightly supervised alignment using BBC TV recordings .<q>CRIM is participating in the speech recognition part of the challenge .<q>CRIM 's best single decoding WER for the MGB-3 dev17 dev set ( with reference segmentation ) went down from 27.6 % ( with our MGB-1 challenge system ) to 24.1 % for this task using the LF-MMI trained TDNN models .
In the Sincerity Sub-Challenge of the Interspeech ComParE 2016 Challenge , the task is to estimate user-annotated sincerity scores for speech samples .<q>Furthermore , we introduce a compact prosodic feature set based on a dynamic representation of F0 , energy and sound duration .<q>We extract syllable-based prosodic features which are used as the basis of another machine learning step .
In addition , the experimental results also show that the RDF has much complementarity with conventional features .<q>Device feature , which contains the information of both recording channel and playback channel , is the critical trait for replay spoofing detection .<q>The experimental result on ASVspoof 2017 corpus version 2.0 shows that equal error rate ( EER ) of replay detection system with the proposed RDF reaches 15.08 % .
The results for perceived urgency also showed that the congruent condition was rated “ evacuate now ” , while the incongruent condition was rated “ wait and see ” .<q>These results suggest that simple combinations of speaking style and textual information decrease the intelligibility of emergency PA announcements , and audio-visual incongruence must be considered .<q>Public-address ( PA ) announcements are widely used , but noise and reverberation can render them unintelligible .
For a fixed number of multitapers , the optimization gives the approximate Wigner distribution of the Gaussian shaped function .<q>High-resolution time-frequency ( TF ) images of multi-component signals are of great interest for visualization , feature extraction and estimation .<q>The matched Gaussian multitaper spectrogram has been proposed to optimally resolve multi-component transient functions of Gaussian shape .
Autism is characterised by socialisation limitations including child language and communication ability .<q>When compared to neurotypical children of the same age these can be a strong indication of severity .<q>We also evaluate deep spectrum features , extracted via an image classification convolutional neural network ( CNN ) from the spectrogram of autistic speech instances .
No study has evaluated the specific use of prosody regarding information structure in the discourse , in French speaking early CI children .<q>Results were interpreted according to both chronological age and hearing age ( HA ) .<q>We conducted a cross-sectional study of 10 prelingually hearing impaired French speaking children ( 4-7 years old ) , without comorbidities , CI before the age of 18 months between 2009 and 2012 .
Classification accuracy results indicate that the glottal parameters contain discriminating information required for the identification of dysarthria .<q>This paper examines the effectiveness of glottal source parameters in dysarthric speech classification from three categories of speech signals , namely non-words , words and sentences .<q>Automatic classification of dysarthria from speech can be used as a potential clinical tool in medical treatment .
First , we demonstrate that caller positions and call timings can be estimated by a sound-imaging method .<q>Such call alternation is reported in various species of frogs including Japanese tree frogs ( Hyla japonica ) .<q>Although our previous study revealed a global synchronization pattern in natural choruses of the male frogs , local chorus structures were not examined well .
In this paper , we propose to use word and phone embeddings to substitute these manual features .<q>A recent approach has reported state-of-the-art performance in speech-to-articulatory prediction using feed forward neural networks .<q>On the other hand , predicting articulatory information from text heavily relies on handcrafted linguistic and prosodic features , e.g. , POS and TOBI labels .
These results indicate a high potential of shadowers ’ facial expressions for comprehensibility prediction .<q>In our previous studies [ 4 , 5 , 6 ] , native listeners ’ shadowing of L2 speech was examined and it was shown that delay of shadowing and accuracy of articulation in shadowing utterances , both of which were acoustically calculated , are strongly influenced by the amount of cognitive load imposed for understanding L2 speech , especially when it is with strong accents .<q>To extract facial expression features , two methods are tested .
These systems are tested on the ASVSpoof 2017 Version 2.0 database .<q>A baseline system is also built using constant-Q cepstral coefficient feature with GMM back-end .<q>A spoof detection system is built using the compressed ILPR feature and a Gaussian mixture model ( GMM ) classifier .
Thus , the proposed tool could also benefit the learners without any access to the effective training methods .<q>Using the front-end , learners can submit their audio to the back-end and can view the corresponding feedback from the back-end .<q>For this , we design a front-end containing self-explanatory instructions that can be easily followed by the user .
This paper presents YIN-bird , a modified version of YIN which exploits spectrogram properties to automatically set a minimum fundamental frequency parameter for YIN .<q>A ground truth dataset of synthetic birdsong with known fundamental frequency is generated for evaluation of YIN-bird .<q>Pitch is an important property of birdsong .
We propose the use of deep autoencoders for the encoding of semantic context while accounting for ASR errors .<q>Deep semantic encodings suppress the noise introduced by the ASR module , and enable SELMs to be optimized adequately .<q>Semantic LMs ( SELMs ) are based on the theory of frame semantics and incorporate features of frames and meaning bearing words ( target words ) as semantic context when training LMs .
In this paper , we utilize an unsupervised learning technique called Bayesian Gaussian process latent variable model ( Bayesian GP-LVM ) to automatically put stress annotation on the given training data .<q>Stress related features are projected onto a latent space in which syllables are easier classified into stressed/unstressed classes .<q>We use the stressed/unstressed information as an additional context in GPR-based speech synthesis .
This paper proposes an approach , named phonetic context embedding , to model phonetic context effects for deep neural network - hidden Markov model ( DNN-HMM ) phone recognition .<q>Then for each speech frame the corresponding DF vector is concatenated with DF vectors of previous and next frames and fed into a neural network that is trained to estimate the acoustic coefficients ( e.g. , MFCCs ) of that frame .<q>In this work they are computed using neural networks .
Voice is projected to be the next input interface for portable devices .<q>The high-level feature vectors derived from this network are used to discriminate between genuine and spoofed audio .<q>Two kinds of low-level features are utilized : state-of-the-art constant-Q cepstral coefficients ( CQCC ) , and our proposed high-frequency cepstral coefficients ( HFCC ) that derive from the high-frequency spectrum of the audio .
Text-independent speaker recognition experiments in Speakers in the Wild ( SITW ) demonstrate a 17.9 % reduction in minimum detection cost with speaker augmentation .<q>This speaker augmentation expands the coverage of speakers in the embedding space in contrast to conventional audio augmentation methods which focus on within-speaker variability .<q>This paper investigates a novel data augmentation approach to train deep neural networks ( DNNs ) used for speaker embedding , i.e .
In this paper , we investigate the impact on articulatory inversion problem by using features derived from the acoustic waveform relative to using linguistic features related to the time aligned phone sequence of the utterance .<q>Experiments are performed on a speech corpus with synchronously recorded EMA measurements and employing a bidirectional long short-term memory ( BLSTM ) that estimates the articulators ’ position .<q>Acoustic FBE features performed better for vowel sounds .
A 208-word ASR experiment using older child speech for training and kindergarten speech for testing was performed to examine the effectiveness of the proposed technique against piecewise vocal tract length , F3-based , and subglottal resonance normalization techniques .<q>The technique is based on the tonotopic distances between formants and fo developed to model vowel perception .<q>Accurate automatic speech recognition ( ASR ) of kindergarten speech is particularly important as this age group may benefit the most from voice-based educational tools .
An orthogonal constraint is imposed in factor matrices for redundancy reduction .<q>Affine transformation in standard neural network is generalized to the spectro-temporal factorization in STFNN .<q>A deep neural factorization is built by cascading a number of factorization layers with fully-connected layers for speech recognition .
Bin-wise time delay is a valuable clue to form the time-frequency ( TF ) mask for speech source separation on the two-microphone array .<q>Time delay histogram is firstly utilized to estimate the delays of multiple sources on each microphone pair .<q>On widely spaces microphones , however , the time delay estimation suffers from spatial aliasing .
In this paper , we investigate SSR based on multiview representation learning via canonical correlation analysis ( CCA ) .<q>This approach was evaluated in a speaker-independent SSR task using a data set collected from seven English speakers using an electromagnetic articulograph ( EMA ) .<q>Articulatory movements generally have less information than acoustic features for speech recognition , and therefore , the performance of SSR may be limited .
Building on existing 2-D models to analyze a spectrogram patch , we propose a multicomponent 2-D AM-FM representation for spectrogram decomposition .<q>This is quantified using reconstruction SNR and perceptual evaluation of speech quality ( PESQ ) metric .<q>The proposed representation provides an improvement over existing state-of-the-art approaches , for both male and female speakers .
This study addresses effects of age and gender on acoustics of European Portuguese oral vowels , given to the fact of conflicting findings reported in prior research .<q>That is , F0 tends to be closer between genders as age increases .<q>Fundamental frequency ( F0 ) , formant frequencies ( F1 and F2 ) and duration of vowels produced by a group of 113 adults , aged between 35 and 97 years old , were measured .
In comparison to web-based crowdsourcing , mobile crowdsourcing is carried out on smartphones or tablets in the field .<q>Secondly , because working times , pauses and work fragmentation can not be controlled , we introduce and focus on the analysis of temporarily expiring training certificates as qualifications .<q>Following up on prior work on assessment of quality of speech in laboratory environments , this paper introduces two recently released mobile crowdsourcing paradigms .
This research extends our earlier work on using machine translation ( MT ) and word-based recurrent neural networks to augment language model training data for keyword search in conversational Cantonese speech .<q>We study how these different methods of language model data augmentation impact speech-to-text and keyword spotting performance for the Lithuanian and Amharic languages .<q>MT-based data augmentation is applied to two language pairs : English-Lithuanian and English-Amharic .
The data also suggest that post-focus compression ( PFC ) is present in Pashto .<q>Basic intonation patterns observed in the language are summarised .<q>The distribution of pitch accent is quite free both in Persian and Pashto , but there is a stronger association of pitch accent with content than with function words , as is typical of stress-accent languages .
Emotional bond is the extent to which the patient feels understood by and connected to the therapist .<q>In particular , we study the complexity in therapist-patient speech as a marker of their emotional bond .<q>First , we extract speech features from audio recordings of their interactions .
A good number of HAs are prescribed but not used partially because of the time to convergence for best operation between the audiologist and user .<q>This may be used to reduce the adjustment cycles for HAs or as preset starting configurations for personal sound amplification products ( PSAPs ) .<q>This study examines a ML clustering method to predict the best initial HA fitting from a corpus of over 90,000 audiogram-fitting pairs collected from hearing centers throughout the USA .
In this paper , we investigate the audible effects of a cold on a phonetic level .<q>mouth and nasal cavities ) .<q>We finally try to predict a speaker ’ s health condition by fusing decisions we derive from single phonemes .
However , HRTFs are highly individual-dependent and thus because of the difference of anthropometric features between subjects , individualization of HRTFs is a great challenge for accurate localization perception in virtual auditory displays ( VAD ) .<q>Furthermore , an iterative data extension method is proposed in order to increase training samples for mapping model .<q>In this paper , we propose a sparsity-constrained weight mapping method termed SWM to obtain individual HRTFs .
Ten native English speaking learners of Mandarin Chinese performed a speeded AX-discrimination task during months 1 , 2 , and 3 of a first-year Chinese course .<q>This study examined how cue-weighting of a non-native speech cue changes during early adult second language ( L2 ) acquisition .<q>Results were compared to ten native Mandarin speakers .
Optimization procedure is crucial to achieve desirable performance for speech recognition based on deep neural networks ( DNNs ) .<q>Experiments on speech recognition using CUSENT and Aurora-4 show the effectiveness of the hybrid accelerated optimization in DNN acoustic model .<q>A new hybrid optimization is proposed by integrating the SGD with momentum and the NAG by using an interpolation scheme which is continuously run in each mini-batch according to the change rate of cost function in consecutive two learning epochs .
Specifically for a TTS , this poses problems as a conventional framework commonly requires the language-dependent contextual linguistics of a full sentence to produce a natural-sounding speech waveform .<q>The Japanese language , which is a mora-timed language , has not been investigated so far .<q>However , most investigations are being done only in French , English and German .
Analytic filter bank provides a simple , fast , and flexible method to construct time-frequency representations of signals .<q>Speech signal consists of events in time and frequency , and therefore its analysis with high-resolution time-frequency tools is often of importance .<q>It is shown that this representation provides comparable , or even better results , than those obtained by spectral magnitude feature vectors in the analysis and classification of vowels .
In this paper , we propose a language-independent end-to-end architecture for prosodic boundary prediction based on BLSTM-CRF .<q>The subjective evaluation results further indicate the effectiveness of the proposed methods .<q>The proposed architecture has three components , word embedding layer , BLSTM layer and CRF layer .
In this work we investigate two models : Static Behavior Model ( SBM ) that assumes a fixed degree of empathy throughout an interaction ; and a context-dependent Dynamic Behavior Model ( DBM ) , which assumes a Hidden Markov Model , allowing transitions between high- and low- empathy states .<q>Empathy by the counselor is an important measure of treatment quality in psychotherapy .<q>We show that the DBM performs better than the SBM , while as a byproduct , generating local labels that may be of use to domain experts .
The second step involves clustering the varying-length segments into a finite number of clusters so that each segment can be labeled with a cluster index .<q>In this paper , we propose a three-step unsupervised approach to zero resource speech processing , which does not require any other information/dataset .<q>In the third step , a deep neural network classifier is trained to map the feature vectors extracted from the signal to its corresponding virtual phone label .
Popular descriptions of posh English mostly focus on vocabulary , accent and phonology .<q>But for the female utterances there was a ceiling effect due to the frequent alternation of speaker gender within the same test session .<q>In four experiments , we tested this hypothesis by acoustically manipulating Cambridge-accented English utterances by a male and a female speaker through PSOLA resynthesis , and having native speakers of British English judge how posh or attractive each utterance sounds .
The current study investigates the listeners ’ preferences on the LC value for IBM processed speech .<q>Ideal binary mask ( IBM ) is a signal-processing technique that retains the time-frequency regions in a mixture of target speech and background noise when the local signal-to-noise ratio ( SNR ) is higher than a local criterion ( LC ) and removes the regions otherwise .<q>For this main cluster of listeners , the preferred LC value depended on the noise type , overall SNR , and the difficulty of the target sentences .
This paper investigates the importance of learning effective representations from the sequences directly in metric learning pipelines for speaker diarization .<q>This method allows speech processing without requiring that an acoustic signal is present ; however , reattachment of the EMG electrodes causes subtle changes in the recorded signal , which degrades the recognition accuracy and thus poses a major challenge for practical application of the system .<q>Experiments are conducted on the CALLHOME conversational speech corpus .
A discriminator in GAN is jointly trained to equalize the difference between real and the predicted data .<q>End-to-end , autoregressive model-based TTS has shown significant performance improvements over the conventional ones .<q>Sentence level intelligibility tests also show significant improvement in a pathological test set .
Based on the amplitude and phase spectra we investigate some possible variations to the extraction of cepstral coefficients that produce diversity with respect to fused subsystems .<q>The goal of this work is to combine ( or fuse ) amplitude and phase-based features to improve speaker verification performance .<q>For performance evaluation , text-dependent speaker verification experiments are conducted on the a proprietary dataset known as Voice Trust-Pakistan ( VT-Pakistan ) corpus .
Though progress in bringing privacy preservation to voice biometrics is lagging behind developments in other biometrics communities , recent years have seen rapid progress , with secure computation mechanisms such as homomorphic encryption being applied successfully to speaker recognition .<q>While still tolerable for single biometric comparisons , most state-of-the-art systems perform some form of cohort-based score normalisation , requiring many thousands of biometric comparisons .<q>This paper proposes the first computationally feasible approach to privacy-preserving cohort score normalisation .
To predict these scores automatically , features based on reading speed and number of disfluencies were extracted , after an automatic disfluency detection .<q>Various regression models were trained , with Gaussian process regression giving best results for automatic features .<q>A computational tool that assists with recording reading tasks , automatically analyzing them and providing performance metrics could be a significant help .
These classifiers are tested on utterances from different English speakers in the TIMIT dataset .<q>Nasals and approximants consonants are often confused with each other .<q>The present study uses a spectral representation obtained using the zero time windowing ( ZTW ) analysis of speech , for the task of distinction between these two .
This also allows us to interpret the weights of the second convolutional layer in the same way as 2D patches learned on critical band energies by typical convolutional neural networks .<q>The evaluation is performed on an English LVCSR task .<q>Trained on the raw time signal , the convolutional layers allow to reduce the WER on the test set from 25.5 % to 23.4 % , compared to an MFCC based result of 22.1 % using fully connected layers .
We consider the task of speech based automatic classification of patients with amyotrophic lateral sclerosis ( ALS ) and healthy subjects .<q>However , a classifier trained with recordings from all devices together performs more uniformly across all devices .<q>Sustained phoneme production ( PHON ) , diadochokinetic task ( DDK ) and spontaneous speech ( SPON ) have been used as speech tasks .
In this paper we substitute the waveform generation vocoder of MUSA , our Spanish TTS , with SampleRNN , a neural vocoder which was recently proposed as a deep autoregressive raw waveform generation model .<q>The subjective evaluation shows that the second system outperforms both the original Ahocoder and SampleRNN as an independent neural vocoder .<q>Secondly , the system is trained with the parameters predicted by MUSA , where SampleRNN and MUSA are jointly optimized .
To improve the robustness to transcription errors , recent solutions propose to map these automatic transcriptions in a latent space .<q>The main drawback of this method is the number of sub-tasks needed to build the c-vector space .<q>Performance of spoken language understanding applications declines when spoken documents are automatically transcribed in noisy conditions due to high Word Error Rates ( WER ) .
Also , we developed a tagger that facilitates the automatic tagging of the code-switching instances .<q>However , for training the language model ( LM ) for such tasks , a very limited code-switched textual resources are available as yet .<q>Code-switching refers to the phenomena of mixing of words or phrases from foreign languages while communicating in a native language by the multilingual speakers .
Physical models of the human vocal tract with a moveable tongue have been reported in past literature .<q>In this study , we developed a new model with a flexible tongue .<q>Apart from the tongue , the model is static and solid ; the gel tongue is the main part that can be manipulated .
Linguistic prosody was accurately matched in all conditions .<q>Matching emotional expressions was excellent for VV , poorer for VA , and near chance for AA and AV presentations .<q>These differences are discussed in terms of the relationship between types of auditory and visual cues and task effects .
This architecture maps a variable length utterance into a fixed dimensional embedding which retains the relevant sequence level information .<q>This kind of network is starting to outperform the state-of-the-art i-vector embeddings in tasks like speaker and language recognition .<q>This is achieved by a temporal pooling layer .
Unfortunately , conventional ASR models are not suitable for the low memory setting of on-device speech recognition .<q>) , for which numeric sequences are composed of in-vocabulary numbers , and then using an FST verbalizer to denormalize the result .<q>In the case of the longest numeric sequences , we see reduction of WER by up to a factor of 8 .
Deep learning is used to estimate the speech presence probability and the update factor of noise statistics , which are then integrated into the Wiener filter-based speech enhancement structure to enhance the desired speech .<q>Noise statistics and speech spectrum characteristics are the essential information for the single channel speech enhancement .<q>Obviously , the hybrid signal processing/deep learning scheme may be a smart alternative .
Deep Neural Networks ( DNN ) have shown promise in a wide range of machine learning tasks , but for Behavioral Signal Processing ( BSP ) tasks their application has been constrained due to limited quantity of data .<q>We present results on multiple behavior codes in the couples ’ therapy domain and demonstrate the benefits in behavior classification accuracy .<q>We also show the viability of this system towards live behavior annotations .
Visual Voice Activity Detection ( V-VAD ) involves the detection of speech activity of a speaker using visual features .<q>In this paper , we propose a speaker independent , real-time solution for V-VAD .<q>Unidirectional LSTMs are used in both the methods to make it online and learn temporal dependence .
To improve the accuracy of the acoustic model , the progressive deep neural networks ( PDNN ) is applied for acoustic modeling in statistical parametric speech synthesis ( SPSS ) in our method .<q>Both objective and subjective experimental results demonstrate the effectiveness of the proposed technique .<q>And the multi-task learning methods were applied to acoustic modeling with several targets in a global cost function .
A key step towards building such a system is to define reliable hotspot labels , which will dictate the performance of machine learning algorithms .<q>A system with this capability can have real applications in many domains .<q>This paper also demonstrates that defining those emotionally salient segments using perceptual evaluation is a hard problem resulting in low inter-evaluator agreement .
Feature selection method is introduced to avoid high variance and overfitting for spoofing detection .<q>In the meanwhile , dimension of the feature is relatively higher than the traditional feature and usually with a higher variance .<q>It has been proven that constant Q cepstral coefficients ( CQCCs ) processes speech in different frequencies with variable resolution and performs much better than traditional features .
A fake speech signal is generated from a very compressed representation of the glottal excitation using conditional GANs as a deep generative model .<q>Classical parametric speech coding techniques provide a compact representation for speech signals .<q>Moreover , the usage of GANs enables to generate signals in one-shot compared to autoregressive generative models .
Then we design a CNN-based feature representation using amplitude and phase information .<q>Integrating amplitude spectrogram with phase information , the relative emotion error recognition rates are reduced by over 33 % in comparison with using only amplitude-based feature .<q>Previous studies of speech emotion recognition utilize convolutional neural network ( CNN ) directly on amplitude spectrogram to extract features .
Singing synthesis is a rising musical art form gaining popularity amongst composers and end-listeners alike .<q>SERAPHIM will be made available as a toolbox on Unity 3D for easy adoption into game development across multiple platforms .<q>SERAPHIM is a wavetable synthesis system that is lightweight and deployable on mobile platforms .
In this work , we present a novel scheme that incorporates depression severity as a parameter in Deep Neural Networks ( DNNs ) .<q>Despite a wide interest in affect prediction , and several studies linking the effect of depression on affective expressions , only a limited number of affect prediction models account for the depression severity .<q>We also present analysis of the impact of such an alteration in DNNs during training and testing .
This study proposes a novel Active Feature Transformation ( AFT ) method for automatic recognition of attitudes ( a form of non-verbal behaviour ) in video blogs .<q>The success of a video blog is measured using metrics like the number of views and comments by online viewers .<q>Researchers have highlighted the importance of non-verbal behaviours ( e.g .
These are naturally-defined objective metrics and are helpful for comparing recognition methods fairly .<q>The latter is close to the averaged correlation between the scores of the human subjects , 0.765 , which suggests that we can predict the human-perceived scores using those features and that we can leverage human perception model for evaluating speech recognition performance .<q>To address this problem , we study and propose a metric which replicates human-annotated scores using their perception to the recognition results .
Voice quality features performed as well as MFCCs .<q>Automatic assessment of depression from speech signals is affected by variabilities in acoustic content and speakers .<q>Leveraging this unique and extensive database , we built an i-vector framework .
In the second step , the syllable and phoneme boundaries and labels are inferred hierarchically by using a duration-informed hidden Markov model ( HMM ) .<q>The proposed method is compared with a baseline method based on hidden semi-Markov model ( HSMM ) forced alignment .<q>We propose a two-step method .
Hidden Markov Models ( HMMs ) have been studied and used extensively in speech and birdsong recognition , but they are not robust to limited training data and noise .<q>For the GMM-HMM framework , the number of states and the mixture components for each state are determined by the acoustic variation of each phrase type .<q>First , the algorithm learns the global Gaussian Mixture Models ( GMMs ) for all training phrases available .
First experiments with an ASR system trained on the Althingi corpus have been conducted , showing promising results .<q>Word error rate of 16.38 % was obtained using time-delay deep neural network ( TD-DNN ) and 14.76 % was obtained using long-short term memory recurrent neural network ( LSTM-RNN ) architecture .<q>Acoustic data acquisition for under-resourced languages is an important and challenging task .
For the training data , we used two well-known meeting corpora - the AMI and the ICSI datasets , together with the provided samples from the DIHARD challenge .<q>The second is a simple noise addition with sampled signal-to-noise ratios .<q>All training setups are compared in terms of diarization error rate and mutual information in the evaluation set of the challenge .
The goal is to provide a speaker with an intuitive visualization of his/her tongue movement , in real-time , and with minimum human intervention .<q>For that purpose , a compact representation of each image is extracted using a PCA-based decomposition technique ( named EigenTongue ) .<q>Artificial neural networks are then used to convert the extracted visual features into control parameters of a PCA-based tongue contour model .
Overall , the results suggest that picture naming and word reading rely on sensory-motor representations that may be linked to contextual ( or surface ) characteristics .<q>This observation contrasts with recent findings showing an effect of the modality ( a written word vs. a go signal ) on adaptation .<q>In this speech production experiment , speakers ’ auditory feedback was altered online , inducing adaptation .
Psychotic disorders are often characterized by two groups of symptoms : negative and positive .<q>Finally , we demonstrate that measures from the Brief Psychiatric Rating Scale ( BPRS ) can be estimated with acoustic descriptors .<q>Our experiments show relationships between psychotic symptoms and acoustic descriptors related to voice quality consistency , variation of speech rate and volume , vowel space , and a parameter of glottal flow .
For four of the six speakers the measures revealed a trend of tenser phonation on the focal syllable ( an increase in EE and F0 and typically , a decrease in OQ and RD ) as well as increased laxness in the postfocal part of the utterance .<q>This paper describes cross-speaker variation in the voice source correlates of focal accentuation and deaccentuation .<q>For two of the speakers , however , the measurements showed a different trend .
The speech overlap may be explained and predicted by the dialog context , the linguistic or acoustic descriptors .<q>Overlapping speech is one of the most frequently occurring events in the course of human-human conversations .<q>Understanding the dynamics of overlapping speech is crucial for conversational analysis and for modeling human-machine dialog .
Speech Enhancement is a challenging and important area of research due to the many applications that depend on improved signal quality .<q>It is a pre-processing step of speech processing systems and used for perceptually improving quality of speech for humans .<q>Experiments are conducted comparing the POS-DAE against the Mean Square Error loss function using speech distortion , noise reduction and Perceptual Evaluation of Speech Quality .
This behavioral study investigated whether and how the frequency and phase of an entrained rhythm would influence the temporal sampling of subsequent speech .<q>Target words were presented at various phases of the entrained rhythm .<q>These outcomes are compatible with theories suggesting that sensory timing is evaluated relative to entrained frequency .
One way to counter such difficulties is through user-machine interaction .<q>It is shown to achieve significantly better performance compared with the previous hand-crafted states .<q>User-machine interaction is important for spoken content retrieval .
Recordings are coarsely synchronized to a common start time .<q>Experimental results show that binary classification accuracy improves from 96.84 % to 97.37 % .<q>Segments of lecture are primarily the speech of the lecturer , while segments of discussion include student speech , silence and noise .
This paper assesses the use of a non-linear manifold structure with multiple DNNs for phone classification .<q>Phone classification experiments are performed on TIMIT .<q>The results show that using the BPC-dependent DNNs provides small but significant improvements in phone classification accuracy relative to a single global DNN .
This paper examined the effect of multi-talker babble noise [ 1 ] on lexical tone identification and discrimination in 14 Cantonese-speaking amusics and 14 controls at three levels of signal-to-noise ratio ( SNR ) .<q>It also affects lexical tone perception .<q>Results reveal that the amusics were less accurate in the identification of tones compared to controls in all SNR conditions .
Example-based speech enhancement is a promising approach for coping with highly non-stationary noise .<q>This paper proposes using bottleneck features ( BNFs ) extracted from a deep neural network ( DNN ) acoustic model for the example search .<q>Experimental results on the Aurora4 corpus show that the example-based approach using BNFs greatly improves the enhanced speech quality compared with that using MFCCs .
Speech recognition of foreign accented ( non-native or L2 ) speech remains a challenge to the state-of-the-art .<q>We investigate both supervised ( where transcription of the accented data is available ) and unsupervised approaches to using the accented data and associated augmentations .<q>The improvements from training accent specific models with the augmented data are substantial .
Engagement is an indicator of how much a user is interested in the current dialogue .<q>We propose a two-step engagement recognition where each annotator 's recognition is modeled and the different annotators ' models are aggregated to recognize the integrated label .<q>The proposed neural network consists of two parts .
